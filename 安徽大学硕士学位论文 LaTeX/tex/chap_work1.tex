\chapter{基于可微分渲染的纹理优化}
\section{本章内容简介}

恢复出高保真的几何模型和纹理一直是三维重建的目标之一，相比几何模型在虚拟现实和游戏等真实领域通常对外观有更高要求。然而，由于数据获取与相机位姿存在各种误差使得重建模型与拍摄彩色图像无法完全对齐，如果不进行纹理优化那么直接生成的纹理会存在错位和缝隙。为了有效解决纹理映射存在的挑战，本文采用优化方法与合成方法共同生成纹理。首先利用可微分渲染生成彩色图片，利用重投影误差优化相机位姿，然后再加权平均所有顶点的投影颜色生成纹理图。单纯的优化相机无法完全对齐纹理与几何，所以本文提出基于对抗生成网络来重生成纹理以适配几何模型。本文在开源数据集上和我们自己拍摄的数据集上进行实验，实验结果表明我们的方法在各种挑战性的数据集上都能生成清晰的纹理。并且对比单纯的优化方法和纹理生成方法我们的方法具有优越性。
\section{引言}


得益于消费机RGB-D相机的普及，人们能够很容易的对真实世界的场景进行建模得到场景和物体的三维模型。近几年，基于RGB-D的三维重建方法取得了前所未有的进步。然而，纹理优化/重建关注度不如几何重建的高，对于游戏、混合现实、虚拟现实等娱乐化数字产业来说纹理细节往往比几何细节更加重要。最近的工作~\cite{RichardNewcombe2011KinectFusionRD,ThomasWhelan2012KintinuousSE,ThomasWhelan2015ElasticFusionDS,SungjoonChoi2015RobustRO,VictorAdrianPrisacariu2017InfiniTAMVA,niessner2013real}已经可以获取高质量几何模型，但是纹理获取结果的质量无法令人满意。\par


造成这一现象的直接原因在于重建模型的几何误差和相机位姿的估计误差，基于RGB-D的三维重建不可避免地会出现几何误差和相机漂移，这些误差会进一步造成最终图像和纹理不能完全与模型对齐，从而造成纹理出现模糊和重影等瑕疵。本质上有以下几点因素使得纹理映射模型结果不尽人意。1）深度相机自身存在镜头误差和采集数据时易受噪声影响使得计算深度距离时出现度量误差。2）重建算法中当前帧的相机位姿估计依赖于前一帧的相机位姿，某一帧相机位姿的估计误差会影响全局进而发生相机漂移或者追踪失败。3）采集的深度图序列和彩色图序列依靠时间戳来描述对应关系，二者相机快门不一定同步会产生错位现象。由于上述错误，平凡地将多个视点图像投影到另一个与视角无关的纹理贴图的纹理映射过程中不可避免地会产生模糊和重影伪影。虽然三维重建技术有着广泛的应用前景，但是由于以上的这些因素很难重建出高保真的三维模型，所以重建的三维模型很难直接应用到其它的领域。\par

由于消费级的深度相机获得的深度图分辨率较低(Kinect第二代深度相机的深度图分辨率为512x424)、噪音大、视野小，测量距离有限。利用这样的深度数据进行重建不可避免的会出现几何细节的扭曲和丢失。此外，我们采用的TSDF 加权的策略对深度数据进行融合，这种加权平均的策略虽然可以过滤掉噪音，但是同时也会平滑掉一些高频几何细节，因此会进一步造成重建模型几何细节的丢失。对于纹理映射，由于相机位姿估计误差及几何误差的存在，不同视口得到的彩色图像不可能完全对齐，所以通过多视口加权生成的纹理不可避免的会出现模糊和重影。为了得到清晰的纹理，先前的工作使用不同的方法来克服重建模型中的误差。Zhou等人~\cite{zhou2014color}采用非刚性抵抗相机位姿存在的误差。Bi等人~\cite{bi2017patch}采用基于块合成方法，重生成纹理图以补偿几何模型和相机位姿中的误差。Fu等人~\cite{fu2018texture}采用全局到局部优化方法以矫正投影矩阵以及纹理坐标。最近Huang等人~\cite{JingweiHuang2020AdversarialTO}借助对抗神经网络，学习误差容忍度量，并使用像素重生成管线重新合成纹理图。\par
虽然神经网络具有强大的拟合能力，对多种不同误差都有很好的抵抗效果。但是当某些误差过大时，重生成的纹理仍旧存在模糊伪影现象。因此我们对相机位姿，几何模型分别进行优化后再借助神经网络重生成具有照片级清晰度的纹理图。\par
基于深度学习的方法在鲁棒性和易用性等方面相对于传统的纹理优化方面有优势，因为让神经网络学习如何抵抗模型重建过程的各种噪声，然后重新生成新的适配与几何模型的纹理。然而合成纹理的效果随着误差值的上升，效果逐渐下降，尤其是在相机位姿估计不准确的情况。某些数据中由于相机估计误差较大，对于几何模型顶点投影不同的彩色图像上，由于错位往往得到不同的颜色，在加权融合后纹理存在模糊伪影，而且单纯用对抗神经网络合成新的纹理图无法根除纹理优化现象。由传统方法Fu等人的工作~\cite{fu2018texture}得到启发，我们将优化算法和纹理合成算法有机地结合起来。首先，借助于梯度下降算法，更新每个视角对应的相机位姿。然后，对每个顶点投影至可见视角中获取的颜色做加权平均算法生成纹理。纹理中存在模糊区域借助对抗生成网络重新合成。\par
传统的相机位姿优化算法往往借助李群和李代数~\cite{sola2018micro}理论优化相机位姿，计算代价高。受可微分渲染在三维单视图模型重建~\cite{liu2020general}~\cite{ShichenLiu2019SoftRA}中的成功启发,借助于可微分渲染我们可以对三维场景的各个组件进行优化，如顶点位置、灯光、相机投影矩阵等。借助于梯度下降算法进行值更新，并且很容易集成到神经网络中而无需额外参数，我们可以计算重投影误差以矫正相机位姿。经过实验证明，我们优化相机后生成的纹理与原始的纹理相比模糊伪影区域显著减少，接近于真实场景的外观。经过对抗生成网络后可以恢复出模型真实表面外观。经过公共数据集和我们自己拍摄的数据集的定量和定性结果比较，我们的算法比单纯用优化法和合成法，性能均有显著提高。
\section{相关工作}
\subsection{纹理优化}
近十几年纹理优化方法被进行广泛的研究，并提出了很多行之有效的算法。本文回顾和总结一些与我们工作相关的纹理优化方法。\par
\vspace*{2mm}\noindent{\bf 基于优化的方法：}生成纹理的方法有加权融合顶点颜色或者投影图像至几何模型，由于不可避免的误差因素存在加权融合算法会产生模糊伪影现象，投影方法图像会产生纹理缝隙。基于优化方法的常常会针对误差项进行优化矫正。Zhou等人~\cite{zhou2014color}优化包括相机姿势和输入图像的非刚性网格变形的参数模型，对纹理图像进行扭曲以适配几何误差。3DLitle~\cite{huang20173dlite}为每个区域选择最佳的视图，以平衡选择不同视图的相邻区域之间边界的视觉清晰度和颜色一致性，利用重建模型中的平面先验来作为约束条件优化相机位姿和纹理。Fu等人~\cite{fu2018texture}提出了提出了一个从全局到局部的非刚性优化策略，利用何一致和颜色一致性约束优化相机位姿消除纹理边界处的细缝并采用扭曲操作额外调整纹理边界顶点对齐，获取全局一致的纹理。


\vspace*{2mm}\noindent{\bf 基于合成的方法：}代替优化法减小误差生成纹理，合成法旨在生成新的纹理图像或者顶点颜色弥补存在的误差。Waechter 等人~\cite{waechter2014let}用基于成对儿的马尔科夫随机场为每个面选择一个最优的纹理图像，又提出全局一致的矫正策略重新生成新的顶点颜色，并用泊松编辑调整纹理细缝处的颜色消除纹理细缝。但是未经过任何的相机位姿和几何模型调整，并不能完全消除细缝。最近，Bi等人~\cite{bi2017patch}针对颜色纹理优化问题，提出了基于块的图像合成策略，在每个视角均合成一张新的目标图像，保留图像原始内容，同时对原始图像发生变形使目标图像之间对齐。该方法有效解决相机估计误差和几何重建误差导致的图像与模型错位问题，但是在每个视角合成新的图像需要耗费很大代价。Huang等人~\cite{JingweiHuang2020AdversarialTO}使用对抗生成网络为整个几何模型生成一张新的纹理图像，通过简单的目标函数训练网络对各种误差项学习容忍度量，相比Bi等人的方法，大大减少了工作量。最近Fu等人~\cite{fu2021seamless}提出新的纹理映射方法，使用一个三向相似度函数来重新合成纹理图边界条纹内的图像上下文，减少纹理接缝的出现。最后引入全局颜色协调方法来解决从不同视点捕获的纹理图像之间的颜色不一致，生成视觉逼真的纹理映射结果。

\subsection{可微分渲染}

网格将3D形状表示为一组顶点和连接这些顶点的曲面。它被广泛使用，特别是在计算机图形学中，因为它可以以紧凑的方式表示复杂的3D形状。在渲染过程中常常会包含深度消隐算法确定离相机最近的三角形。由于会产生离散的三角形标识符而造成渲染过程的不可微。一些方法采取梯度近似方法，使用手工设计的梯度Loper和Black等人~\cite{MatthewLoper2014OpenDRAA}提出OpenDR的通用可微分渲染器，使用近似的空间梯度。Kato等人~\cite{MatthiasNiener2013Realtime3R}揭示了OpenDR中存在的两个问题并提出了一种名为神经3D网格渲染器(NMR)的渲染器，使用非局部近似梯度并包含从损失函数反向传播的像素的梯度，克服OpenDR优化过程中易陷入局部最小值的情况。Genova等人~\cite{KyleGenova2018UnsupervisedTF}使用每个三角形相对于每个像素的重心坐标计算光栅化导数。它们为位于三角形边界外的像素的重心坐标引入负值，以克服遮挡不连续性。另一些方法不是近似渲染后向过程，而是近似渲染的光栅化(或前向过程)，以便能够计算有用的渐变。Rhodin等人~\cite{HelgeRhodin2015AVS}重新解释场景参数以确保渲染可微分。Liu等人~\cite{ShichenLiu2019SoftRA}采取类似的方法，提出一个名为Soft Rasterizer的渲染器以概率分配形式代替传统深度消隐过程。Chen等人~\cite{chen2019_dibr}提出了DIB-R，它独立地聚焦于图像的两个不同区域：前景像素，背景像素，对前景像素使用解析导数，对背景像素采用基于距离的全局面信息聚合来进行前向渲染。我们在这项工作中采用了Soft Rasterizer，它允许我们通过在渲染图像和相应的RGB-D输入之间强制弱监督损失来优化3D模型的参数，包括纹理、几何和相机位姿。
\subsection{对抗生成网络}
生成对抗网络GAN~\cite{NIPS2014_5ca3e9b1}通过生成器与判别器的零和博弈以生成符合真实示例的图像。GAN网络优秀的表现性能使得能在视觉领域大放异彩，如图像翻译~\cite{isola2017image}、阴影消除~\cite{fu2021auto}、高光去除~\cite{fu2021multi}、新视角合成~\cite{sauer2023stylegan}、图像修复~\cite{yu2018generative}等领域取得了令人深刻的结果。由于纹理图像和采集数据存在差异并且纹理映射使得优化过程不稳定，因此很难把对抗损失包含在内。Huang等人~\cite{JingweiHuang2020AdversarialTO}首次将对抗生成引入纹理优化领域，消除了传统的L2损失对颜色、照明或清晰度差异不稳健的影响，合成高频细节的清晰纹理图像。相比于块合成的方法~\cite{bi2017patch}，通过视图变换弥补误差问题，扩展性更强。

%%缺流程图
\section{算法流程}
我们的方法旨在通过RGB-D相机采集的深度图片和彩色图片，并将不同视角的彩色图适配到几何模型上以生成具高保真的纹理贴图。为了达成这一目标，我们提出结合相机位姿优化与纹理生成算法提高纹理模型的清晰度和保真度。该算法的具体流程分为四个步骤。
\noindent\textbf{模型重建：}RGB-D相机采集的原始数据为彩色图序列和深度图序列。在公共数据集上，一般会提供深度图集、彩色图集、对应的相机内外参以及用重建算法~\cite{SungjoonChoi2015RobustRO,AngelaDai2016BundleFusionRG,RichardNewcombe2011KinectFusionRD}重建出的初始几何模型。在我们自己拍摄的数据集上我们需要自己重建几何模型以及估计对应的相机位姿。我们利用重建算法~\cite{LongYang2018SurfaceRV}重建初始几何模型，并估计初始的相机位姿。\par
\noindent\textbf{预处理：}在预处理阶段，我们从所有帧中选出最清晰的彩色图像当作候选图像，消除冗余和重叠度较高的视角。同时保证所选关键帧的视角覆盖范围和原始集合一致。我们额外采用多视觉进行监督，避免单视角可能因相机飘移导致异常值影响纹理优化，我们为每一帧都选取邻近视角，同时用源视角和扭曲后的视角进行监督。\par
\noindent\textbf{相机位姿优化：}为了借用可微分渲染框架优化相机位姿，首先渲染三维模型至某一视角的彩色图像，然后再用真实图像与渲染图像做图像间的损失以优化相机投影矩阵。优化相机位姿完成后，我们将模型顶点投影至所有可见视角获取顶点颜色，再用加权平均算法融合颜色以生成纹理，为了便于后续纹理优化我们将颜色以UV纹理形式保存。\par
\noindent\textbf{纹理优化：}由图像翻译模型~\cite{isola2017image,wang2018high}得到启发，我们借助对抗生成网络在初始纹理基础上重生成新的UV纹理，我们已知纹理到图像的映射关系，并且获取相对准确的相机位姿。用源视角$I_A$的邻近视角$I_B$扭曲到源视角当作真例，渲染图像为假例，以对抗生成模型优化纹理图。\par

\subsection{数据预处理}
\noindent\textbf{重建模型：}RGB-D数据我们使用消费级深度相机kinect V1或者kinect v2在固定曝光和白平衡模式下拍摄。拍摄完成后已知信息只有初始拍摄所得的为深度流和彩色流以及相机标定后的内参，并没有相机位姿与重建模型。由最近纹理优化方法G2Tex~\cite{fu2018texture}得到启发，G2Tex算法输入是深度相机扫描的深度图集合和彩色图集合，借助基于RGB-D三维重建算法~\cite{LongYang2018SurfaceRV}利用稀疏序列融合（Sparse-Sequence Fusion，SSF）策略并加以改进，获取高可用、高质量的RGB-D序列重建TSDF模型，并用移动立方体算法抽取出网格模型。重建几何模型选帧策略依照如下公式进行：
\begin{align}
E(i)= \lambda_{1} E_{\mathrm{jit}}(i)+\lambda_{2} E_{\mathrm{dif}}(i)+\lambda_{3} E_{\mathrm{vel}}(i)+\lambda_{4}E_{\mathrm{cla}}(i)+E_{\mathrm{sel}}(i)
\end{align}
其中$E_{\mathrm{sel}}(i)$当前帧$i$是否作为三维重建的有效帧的控制项，$E_{\mathrm{jit}}(i)$衡量当前帧$i$与前一个有效帧的抖动强度比较，尽量保证选取平滑非抖动帧进行重建，$E_{\mathrm{dif}}(i$确保前后有效帧间有超过阈值重叠覆盖率，避免相机跟踪失败，$E_{\mathrm{vel}}(i)$度量相机运动速度，避免相机运动过快产生的模糊帧，$E_{\mathrm{cla}}(i)$是图像清晰度度量指标，衡量彩色图片清晰度以保证选择的彩色图像最清晰。经过重建算法后我们得到初始网格模型$M$。\par
\noindent\textbf{关键帧获取。}由于RGB-D相机是手持进行拍摄，拍摄过程中不可避免的出现抖动相机运动过快，往往会造成所拍摄图片发生模糊、失真等现象。为了根除这个现象，我们会根据 Crete 等人提出的图片的模糊度度量指标 ~\cite{FrederiqueCrete2007TheBE}评估每一帧。对于彩色图超过100张的数据集，经验上我们设置一个尺寸为5的滑动窗口，在每个窗口内选择模糊度最小的图片作为关键帧$KF$，根据数据集的大小灵活调整窗口尺寸。经过实验证明选择关键帧并不会影响优化效果而且会线性减少优化时间。 \par
\noindent\textbf{辅助视角选取。}受Huang等人~\cite{JingweiHuang2020AdversarialTO}的启发，我们为每个原始视角$T_s$选取一个辅助视角$T_t$,使得辅助视角重投影至原视角作为真例以此来监督渲染的图片。令$p_s$，$p_t$分别表示原始视角和目标视角的齐次坐标，则二者的对应关系可以描述为:
\begin{align}
	p_s\sim KT_{t\rightarrow s}D_tK^{-1}p_t \label{work1:wrap}
\end{align}
其中$D_t$表示目标视角的深度值。由于遮挡和深度图存在噪声的缘故，目标视口扭曲到原始视角因未对齐而产生残缺现象。为了防止残缺部分过大，我们为任意两个视口计算z方向夹角$\theta$，当$\theta\le15^{\circ}$时两个视角才被符合源视角-目标视角位姿对儿集合。

\subsection{相机参数优化}
在本节中，我们详细介绍相机优化原理与步骤。刚体变换的旋转表示有旋转矩阵、欧拉角、轴角表示、四元数等。我们使用旋转矩阵，一方面易于向其他表示转换，另一方面很容易在笛卡尔坐标系下变换顶点位置。令$ \left \{ \mathbf{I_i} \right \} $表示输入图像集合，$\left \{ \mathbf{T_i} \right \}$表示相机位姿集合，$\left \{ \mathbf{P_i} \right \}$表示图像对应的模型顶点集合。旋转矩阵定义为:
\begin{align}
	T = \left[\begin{array}{cc}
		R & t \\
		0 & 1
	\end{array}\right]
\end{align}
$T \in \mathrm{SE} (3)$，$R \in \mathrm{SO}(3)$并且$t\in\mathbb{R}^3$，其中$T \in \left \{ \mathbf{T_i} \right \}$为4$\times$4矩阵。\par 
三维重建模型定义在世界坐标系下，并以模型中心为坐标原点。经过刚体变换后，以相机所在位置为坐标原点，模型处于在相机正前方。然后我们只需要将相机坐标系下的模型顶点投影至二维图像平面上即可。根据相机成像模型定义刚性变换为$\mathbf{G}(p)=Tp, p \in \left \{ \mathbf{P_i} \right \}$，变换后的顶点齐次坐标为$\mathbf{g}=[g_x, g_y, g_z,g_w]^\top$。设投影图像平面变换为$\mathbf{U}$，我们按照以下公式计算图像平面中的像素位置：
\begin{align}
\mathbf{U}\left(g_{x}, g_{y}, g_{z}, g_{w}\right)=\left(\frac{g_{x} f_{x}}{g_{z}}+c_{x}, \frac{g_{y} f_{y}}{g_{z}}+c_{y}\right)^{\top} \label{project}
\end{align}

其中，$f_x,f_y$表示相机的在$x,y$方向的焦距长度，$c_x,c_y$分别表示相机的光心坐标。这些参数可以利用相机标定获取。\par

在基于RGB-D的三维重建中一般使用迭代最近点（Iterative Closest Point，ICP）算法~\cite{besl1992method,chen1992object}及其衍生算法估计初始的相机位姿。由于存在噪声相机位姿估计存在累计误差，即使有回环检测也并不都能完全消除。使用不完美的相机位姿渲染图片，渲染图片和采集图片会发生错位现象。在传统的优化相机方案中，通常计算网格顶点重投影误差来优化相机投影矩阵$T$。设模型上一顶点$p$，初始颜色为$C(p)$（初始颜色，利用顶点投影后的像素颜色平均可得），定义$\Gamma(x,y) $为灰度图像上$(x,y)$位置的颜色值。顶点颜色与投影至视角$i$上的像素颜色值会存在差值，定义残差项为：
\begin{align}
	R_{i, \mathbf{p}}=C(\mathbf{p})-\Gamma_{i}\left(\mathbf{U}\left(\mathbf{G}\left(\mathbf{p}, \mathbf{T}_{i}\right)\right)\right)
\end{align}
同时定义优化目标为
\begin{align}
	E(\mathbf{T})=\sum_{i} \sum_{\mathbf{p} \in \mathbf{P}_{i}} R_{i, \mathbf{p}}^{2}
\end{align}
利用高斯牛顿优化方法~\cite{wedderburn1974quasi}对上述公式进行迭代求解，并且更新一次相机位姿$T$后，重新计算每个顶点的颜色$C(p)$。最终获得精确的相机位姿以及帧间一致的顶点颜色。由传统优化方法得到启发，我们借助于可微分渲染框架以投影顶点至图像平面，使用UV纹理保存顶点颜色，并使用$L_1$范数度量图像间的颜色差异，最后使用梯度下降算法朝向目标方向优化相机位姿矩阵$T$。\par
基于渲染能力和效率的考量，并且我们无需对模型表面材质进行额外建模。所以使用基于光栅化的渲染框架~\cite{ravi2020pytorch3d}和布林冯着色模型，渲染目标遵从以下目标：
\begin{align}
	C_i = (I_a + I_d) * P_i + I_s
\end{align}
其中$I_a,I_d,I_s$分别表示环境光、漫反射光和高光项，$C_i,P_i$分别表示彩色图像素和纹理像素。为了最大限度的还原具有保真度的纹理，我们只保留环境光，设置高光和漫反射光系数为0。对于相机外参T，我们采用以上介绍的投影和表示方法。 由于渲染框架中所有模块都是可微分的，并可以嵌入到深度网络中，从而实现端到端的训练。具体地，每一次渲染我们会随机选择某个视角，在该视角下生成彩色图$\tilde{I}_c$深度图$\tilde{I}_d$以及阴影图$\tilde{I}_s$即$I_c,I_d,I_s = Render(M_0|T_i)$。我们首先使用光度一致性损失来优化已选帧的相机位姿
\begin{align}
	E_c = \left \| I_c - \tilde{I}_c  \right \|_1 
\end{align}
然而仅仅具有光度一致性损失不足以约束相机位姿以朝着我们期望的方向优化，在某些场景中纹理较为单一或者纹理细节较少，相机仍会发生漂移现象。因此我们额外考虑深度图损失来增强几何一致性。
\begin{align}
	E_d = \left \| I_d - \tilde{I}_d  \right \|_1 
\end{align}
受SoftRas~\cite{ShichenLiu2019SoftRA}启发我们也采用轮廓损失来对几何进行约束。这个阴影损失函数为：
\begin{align}
	E_s = 1 - \frac{\left \| \hat{I_s}\otimes I_s  \right \|_1 }{\left \| \hat{I_s}\oplus  I_s- \hat{I_s}\otimes I_s \right \| }_1  
\end{align}
其中$\otimes $和$\oplus $分别是按元素计算的乘法运算符和运算符。最后我们按照每个损失对优化相机参数的贡献来赋予每个损失不同的权重。
\begin{align}
	L_T = \lambda_c E_c + \lambda_d E_d +\lambda_s E_s \label{pose}
\end{align}经验上我们设置$\lambda_c = 0.1$，$\lambda_d = 1$，$\lambda_s = 1$。\par
注意，在不同场景下我们可能微调优化相关的超参数，并且相机位姿本身没有真值可得。评估相机位姿是否准确的唯一方案就是利用优化后的相机位姿$T'$生成新的纹理图，并比较与原始纹理之间的清晰度，越清晰则优化效果越好。

%缺网络架构图和描述
%
\subsection{纹理重合成}


仅仅只矫正相机位姿是不够的，优化结果并不能完全消除噪声。传统方法中单纯优化相机步骤后，结果并不能使人满意，往往需要结合网格细分或者扭曲纹理图像或者坐标使得进一步使得图像与几何模型对齐。本质上都是抵抗重建模型的噪声以弥补几何误差带来的影响。对于我们的纹理优化算法也存在这个问题，况且我们采用顶点加权融合方式生成的纹理在局部区域仍存在模糊并且缺失细节。由Huang等人~\cite{JingweiHuang2020AdversarialTO}得到启发我们用对抗生成网络学习重建模型和拍摄的彩色图片错位的容忍度。我们使用基于$\pi$GAN~\cite{chanmonteiro2020pi-GAN,liu2018intriguing}的网络用像素重生成管线合成纹理图。网络结构类似于pix2ix~\cite{isola2017image}的条件对抗网络，利用判别器$D$学习区分真图片和渲染图片，并产生多视角一致性的纹理图像。我们使用辅助视角重投影至原视角$I_c^{B\to A}$为真例监督可微分渲染生成的彩色图假例以更新纹理，同时设置源图像为条件。相应地对抗损失函数定义如下：
\begin{align}
	L_{a d v}=\log D\left(\boldsymbol{I}_{A}, \boldsymbol{I}_{B \rightarrow A}\right)+\log \left(1-D\left(\boldsymbol{I}_{A}, \boldsymbol{I}_{A}^{D R}\right)\right) 
\end{align}
其中$\boldsymbol{I}_{A}^{D R}$表示基于源视角的渲染图像。为了使得更稳定的进行对抗生成纹理优化，我们额外增加L1损失避免优化过程中陷入局部最小值，并为优化提供初始的指导。定义L1损失如下：
\begin{align}
L_T = \left \| \boldsymbol{I}_{A}- \boldsymbol{I}_{A}^{D R} \right \| 
\end{align}
最终我们的优化目标为:
\begin{align}
	L = L_{adv} + \lambda L_T \label{texture}
\end{align}
在优化期间，我们设置纹理图像$P$背景像素值为0，并且设$\lambda$为10。我们会每隔1000轮次会指数衰减$\lambda$的数值。\par
具体地，对于每一次优化迭代，我们随机选择两个输入图像，$I_A$(源图像)和$I_B$(辅助图像)，以及对应的相机姿态$T_A$和$T_B$，$I_A$为条件。“真实图像”为按照公式\ref{work1:wrap}将辅助图像扭曲到源视角后得到图像$I_{B\to A}$。由于遮挡和相机位姿未对齐等原因，$I_{B\to A}$图像存在残缺现象并且未残缺的部分与源图像$I_A$相比像素会发生轻微的移位导致未对齐。“伪”图像是根据当前的相机位姿$T_A$从三维纹理模型渲染形成的图像。纹理的模糊伪影会反映至渲染图像上，判别器识别伪图像和真实图像间的差异，生成器重新生成纹理像素使得渲染图像和扭曲图像充分接近以匹配图像视角与纹理模型的映射关系。在纹理优化过程中，我们调整纹理像素的颜色，最大化对抗性损失，使其在鉴别器评分下看起来更逼真。在鉴别器优化过程中，我们将对抗性损失最小化，从而更好地对真假样本进行分类。我们将对抗性损失与L1损失进行线性组合，有助于提高纹理优化效果。\par
经过若干次迭代交替训练判别器$D$和纹理$P$后，判别器会正确识别渲染图片中存在的伪影模糊或者裂缝，纹理图像会被重新生成像素以弥补渲染图片和真实图片之间错位现象，使得两者充分接近以达到愚弄判别器的目的。经过优化后的纹理相比与用加权融合方法生成的纹理更加真实，更接近于真实拍摄的彩色图片。因此，被优化以愚弄鉴别器的纹理看起来比仅优化相机位姿生成的纹理更逼真。\par


\subsection{交替优化}
我们并不将优化方法和合成方法集成到一个模型中。相反，我们采用交替优化方案，先优化相机位姿后合成纹理图像，这有助于为寻找良好的初始纹理。一般来说优化相机位姿和对抗生成网络合成纹理顺序可以交换，但是我们经验上发现，先优化相机姿态更有助于提高纹理的保真度。一方面是相机位姿是纹理映射的核心，若相机位姿完全无误差则不同视角直接贴在几何模型中就可以生成完美的纹理图。换句话说，位姿对纹理生成的影响最大。另一方面，合成纹理图也依赖于位姿变换，位姿越准确合成的效果越好。在外部迭代中，我们优化相机位姿后再
合成纹理模型并且整个优化过程重复$N$次，内部迭代中我们根据不同场景设置迭代次数$S$。一般情况下我们设置$N = 1$，即可生成清晰的纹理图像。


\section{实验}
\paragraph*{比较方法}
在这个部分我们会和最新的纹理优化方法G2Tex~\cite{fu2018texture}，ATO~\cite{JingweiHuang2020AdversarialTO}，zhou~\cite{zhou2014color}在公开的RGB-D数据集上并用他们在GitHub提供的源代码在进行比较。然后在我们自己拍摄的RGB-D的数据集上进行比较，我们在每个数据集的多个场景中进行定性及定量的实验分析证明我们的方法的有效性。数据集中既包含小场景也包含大场景，既含有丰富纹理细节的场景，也包含单一纹理及弱纹理的场景。最后，我们会展示消融研究以证明我们优化方案中各个组件的必要性。

\paragraph*{评估指标}

为了对所生成的纹理进行定量的研究，我们采用多种指标来衡量纹理和真实图片之间的差异。目前纹理优化结果并没有行之有效的定量评估指标，一方面，因为大部分纹理映射算法会采用wrap图像的方法以适配
几何模型，导致完全使用采集的图像作为评估标准有待商榷。另一方面，纹理映射结果表示形式多样，没有一个标准的适用于对比的模型。所以评估纹理映射的好坏主要以定性比较为主。但是定量评估指标能够在一定程度上反应纹理图像的清晰度和保真度。我们使用经典的图像间的评估指标来作为定量分析结果，如峰值信噪比（Peak signal-to-noise ratio ，PNSR）~\cite{de2003improved}用于量化受损压缩影响的图像，能够反应图像的保真度。结果越高代表重建图像质量越好；结构相似性 (Structural similarity index，SSIM)~\cite{brunet2011mathematical}衡量两张图片的相似程度，符合人类对图像的视觉感受，评价结果过高越好；学习感知图像块相似度(Learned Perceptual Image Patch Similarity, LPIPS)~\cite{zhang2018unreasonable}也称为感知损失，以神经网络计算误差方法度量两张图像之间的差别，更符合人类的感知其结果越小越好。我们渲染纹理模型至不同视角，并使用所有视角的平均值来作为最后的结果。


\paragraph*{训练设置}
本章方法使用Ubuntu Server机器进行训练和测试，显卡为单个NVIDIA GeForce RTX3090 24GB，在训练网络时使用Adam优化器训练模型，并设置超参数$\beta_1=0.5，\beta_2=0.999，\epsilon =10-8$。在优化相机位姿时设置初始学习率为0.0001，用对抗生成优化纹理时设置初始学习率为0.01。


%
% zhou、chair00、chair11、chair26、chair29、chair34, 定性定量指标
\subsection{在公共数据集上进行评估}
我们首先在ATO~\cite{JingweiHuang2020AdversarialTO}发布的椅子数据集上数据集上数据集上进行实验比较。如图X所示，可以看出我们的方法得到了最佳的效果，ATO采用对抗神经网络方法合成纹理图，虽然能够容忍较小的相机误差和几何误差，但是在局部地方仍然有模糊现象。G2Tex采用面投影方法，并优化了相机位姿，能够产生的纹理整体比较清晰，但是局部地方会有明显裂缝出现，这一点纹理比较丰富的数据集上更加明显。如图A，在花纹明显的椅子上，ATO合成了较为清晰的纹理图像但是在花瓣处的仍旧存在模糊现象，我们的方法由于先优化了相机位姿，为模型提供了更正确的纹理映射。所以无论从整体还是局部更符合真实场景。图B会有细小的花纹，ATO未能合成全部图案，特别是局部细节较为模糊。G2Tex由于采用面投影方法所以整体非常清晰但是会有局部细缝出现，在含有较细线条的场景下尤为明显。我们的方法整体视觉效果最好。其它场景下纹理较为单一，虽然三者优化结果类似但是我们的方法在局部区域不会有明显的模糊现象。我们额外进行了定量评估，生成模型后我们在所有视角下评估定量指标最后取平均值展示。如表所示，我们的结果在椅子数据集上的定量指标（如上所示的）均超过了，最新的纹理优化方法。\par

最后，我们比较各种基线方法在zhou等人~\cite{Zhou2018}发布的Fountain数据集，该数据集视角非常稀疏。在此数据集上，我们的方法可以很好的处理标准化数据集，如图所示。我们的方法在几何细节和纹理细节方面对比其他方法展示出优越性。如图红色框中所示，我们的方法能恢复出清晰的喷泉模型文字，产生一致的纹理。其他方法在局部区域要么会产生模糊伪影要么会产生不一致的边界如G2Tex。ATO虽然能合成一致的边界但是在右侧区域看起来非常模糊，由于该数据集中相机位姿误差较大虽然对抗生成网络能抵抗一部分噪声，但是仍旧存在瑕疵。而我们的方法避免了这个问题。
我们额外实现了定量评估，如表所示，我们的方法对重建误差进行矫正，再用像素重生成管线合成纹理图，对几何误差和相机位姿误差更具有鲁棒性。经过联合优化后重建出的纹理，渲染的图片更接近于真值。\par

% desk与bloster1模型及其它模型结果
\subsection{收集数据上进行评估}

我们提供了自己用Kinect一代相机拍摄的数据集，它获得的深度图和彩色图 的分辨率较低 (640 × 480)，所以在获得的彩色图像中很容易出现模糊和细节丢失的情况。我们在多个具有挑战性的数据集上，如含光照变化，阴影等场景，进行定性和定 量分析来证明我们方法的有效性。\par
如图C所示，我们的算法提供了全局一致性的纹理，即使重建模型体积微小，细节处仍能保证清晰度。如黄色框所示，G2Tex会出现纹理细缝问题，我们的方法看起来较为平滑。人物正身处的文字、以及书籍名称经过ATO合成方法后文字看起来较为模糊，我们和G2Tex方法无此现象。图D所示，该数据集相机位姿误差较大，没有优化情况下，纹理图像非常模糊。即使用对抗生成网络重合成，但是也无法弥补图像与模型错位现象。G2Tex虽然也进行了相机优化，但是图案间仍然会有明显的裂缝，经过我们的相机优化成功抵抗了噪声影响，所以最终恢复了所有纹理图案，虽然局部区域仍会有模糊现象但是整体质量明显高于ATO。最后我们采用定量分析法，对整个纹理模型渲染的图像用上述介绍的评估指标进行考量，我们的数据略高于两者。


%%
% 分别研究损失函数、仅深度图、仅彩色图。分别研究没有优化相机位姿、没有对抗生成网络情况，并添加随机噪声结果
%
\subsection{消融研究}
在这个部分，我们将研究方法中各个部分的有效性。为了更有说服力，我们选择真实场景中的数据集Fountain。首先我们移除各个部分，没有优化相机位姿，没有对抗生成网络。\par
如图所示，我们展示了移除的各个部分效果。没有矫正相机位姿情况下，渲染图片和真实图片会有错位现象，借助于对抗生成我们仍都能得到清晰的纹理，但是缺乏细节。没有对抗生成网络时，无论是纹理细节还是整体视觉效果都明显下降，并在定量评估中我们同样得出了一致的结论。\par


\subsection{算法局限性}
我们的优化算法虽然已经取得了一定的成效但是仍旧有以下不足的地方。1）无法自定义纹理坐标，我们使用obj格式保存三维模型，需要纹理坐标表示几何与纹理图像（UV纹理）的映射关系。但是ply存储格式转为obj格式时通常并不会额外生成纹理坐标而我们的算法不包含纹理坐标产生流程，所以限制了我们的方法通用性。2）需要数据量大，我们遵循深度学习为主的优化思路需要较大的训练集来更新相机位姿与纹理图像，传统方法只需要覆盖模型$360^{\circ} $的图像即可，小数据集会导致我们的优化效果不理想情况，这也限制了我们在小场景下的应用。3）某些场景下，几何模型存在较多误差，即使优化纹理图像也无法保证与几何模型的正确映射关系，尤其是在几何模型存在孔洞和非流形拓扑结构情况下。




\section{本章小结}

本章，我们提出了针对RGB-D三维重建模型的优化与合成方法结合的纹理优化算法。针对基于RGB-D三维重建产生的模糊纹理问题；首先，我们输入三维重建的模型与相机采集图片序列及其位姿，用可微分渲染渲染纹理至图像，使用重投影误差产生梯度更新相机投影矩阵；然后，用对抗生成网络重新合成新的纹理图像以适配重建模型；最后，得到一个完整的纹理模型。通过纹理优化后，我们获取了准确的相机位姿并以像素重生成管线弥补其它误差项，从而在各种误差因素下仍旧得到全局一致的纹理模型。实验结果表明我们的算法无论在大场景和小场景中还是在含有丰富纹理细节和含有弱纹理场景下，都能恢复出高保真的纹理，也证明了我们算法的高通用性和强鲁棒性。



