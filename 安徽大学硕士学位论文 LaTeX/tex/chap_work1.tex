\chapter{基于可微分渲染的纹理优化}
\section{本章内容简介}

恢复出高保真的几何模型和纹理一直是三维重建的目标之一，相比几何模型在虚拟现实和游戏等真实领域通常对外观有更高要求。然而，由于数据获取与相机位姿存在各种误差使得重建模型与拍摄彩色图像无法完全对齐，如果不进行纹理优化那么直接生成的纹理会存在错位和缝隙。为了有效解决纹理映射存在的挑战，本文采用优化方法与合成方法共同生成纹理。首先利用可微分渲染生成彩色图片，利用重投影误差优化相机位姿，然后再加权平均所有顶点的投影颜色生成纹理图。单纯的优化相机无法完全对齐纹理与几何，所以本文提出基于对抗生成网络来重生成纹理以适配几何模型。本文在开源数据集上和我们自己拍摄的数据集上进行实验，实验结果表明我们的方法在各种挑战性的数据集上都能生成清晰的纹理。并且对比单纯的优化方法和纹理生成方法我们的方法具有优越性。
纹理，
\section{引言}
得益于消费机RGB-D相机的普及，人们能够很容易的对真实世界的场景进行建模得到场景和物体的三维模型。近几年，基于RGB-D的三维重建方法取得了前所未有的进步。然而，纹理优化/重建关注度不如几何重建的高，对于游戏、混合现实、虚拟现实等娱乐化数字产业来说纹理细节往往比几何细节更加重要。最近的工作已经可以获取高质量几何模型，但是纹理优化的质量无法令人满意。造成这一现象的根源在于重建模型的几何误差和相机位姿的估计误差，基于RGB-D的三维重建不可避免地会出现几何误差和相机漂移，这些误差会进一步造成最终图像和纹理不能完全与模型对齐，从而造成纹理出现模糊和重影等瑕疵。\par
要通过RGB-D相机获得高质量、高保真的三维重建模型，必须达到两个最基本的需求:物理意义正确并且高质量的几何模型和高保真度照片级别的纹理。然而，这两个基本需求很容易受到以下因素的影响和破坏:(1）深度相机获取的深度时很容易受到噪音、镜头扭曲和量化误差等因素的影响,造成深度测量出现度量误差。(2)在相机位姿估计时很容易出现相机累积误差造成相机位姿漂移。(3)现有的三维重建算法普遍采用TSDF(truncated signed distance field)数据融合技术来进行三维数据融合和表示，它虽然可以利用加权平均来消除大部分的噪音，但是它同时也会平滑掉很多高频的几何细节使得模型趋于平滑，而造成重建的模型出现几何误差(与真实几何模型之间的差异)。由于三维重建中几何误差和相机漂移，最终重建的纹理不可避免地会出现模糊和重影等瑕疵。这些因素导致重建的几何模型和纹理结果都无法满足当前一些应用的高质量几何和高保真纹理的需求。虽然三维重建技术有着广泛的应用前景，但是由于以上的这些因素很难重建出高保真的三维模型，所以重建的三维模型很难直接应用到其它的领域。\par

由于消费级的深度相机获得的深度图分辨率较低(Kinect第二代深度相机的深度图分辨率为512x424)、噪音大、视野小，测量距离有限。利用这样的深度数据进行重建不可避免的会出现几何细节的扭曲和丢失。此外，我们采用的TSDF 加权的策略对深度数据进行融合，这种加权平均的策略虽然可以过滤掉噪音，但是同时也会平滑掉一些高频几何细节，因此会进一步造成重建模型几何细节的丢失。对于纹理映射，由于相机位姿估计误差及几何误差的存在，不同视口得到的彩色图像不可能完全对齐，所以通过多视口加权生成的纹理不可避免的会出现模糊和重影。为了得到清晰的纹理，先前的工作使用不同的方法来克服重建模型中的误差。zhou等人~\cite{zhou2014color}采用非刚性抵抗相机位姿存在的误差。bi等人~\cite{bi2017patch}采用基于块合成方法，重生成纹理图以补偿几何模型和相机位姿中的误差。fu等人~\cite{fu2018texture}采用全局到局部优化方法以矫正投影矩阵以及纹理坐标。最近huang等人~\cite{JingweiHuang2020AdversarialTO}借助对抗神经网络，学习误差容忍度量，并使用像素重生成管线重新合成纹理图。\par
虽然神经网络具有强大的拟合能力，对多种不同误差都有很好的抵抗效果。但是当某些误差过大时，重生成的纹理仍旧存在模糊伪影现象。因此我们对相机位姿，几何模型分别进行优化后再借助神经网络重生成具有照片级清晰度的纹理图。\par
基于深度学习的方法在鲁棒性和易用性等方面相对于传统的纹理优化方面有优势，因为让神经网络学习如何抵抗模型重建过程的各种噪声，然后重新生成新的适配与几何模型的纹理。然而合成纹理的效果随着误差值的上升，效果逐渐下降，尤其是在相机位姿估计不准确的情况。某些数据中由于相机估计误差较大，对于几何模型顶点投影不同的彩色图像上，由于错位往往得到不同的颜色，在加权融合后纹理存在模糊伪影，而且单纯用对抗神经网络合成新的纹理图无法根除纹理优化现象。由传统方法fu等人的工作~\cite{fu2018texture}得到启发，我们将优化算法和纹理合成算法有机地结合起来。首先，借助于梯度下降算法，更新每个视角对应的相机位姿。然后，对每个顶点投影至可见视角中获取的颜色做加权平均算法生成纹理。纹理中存在模糊区域借助对抗生成网络重新合成。\par
传统的相机位姿优化算法往往借助李群和李代数~\cite{sola2018micro}理论优化相机位姿，计算代价高。受可微分渲染在三维单视图模型重建~\cite{liu2020general}~\cite{ShichenLiu2019SoftRA}中的成功启发,借助于可微分渲染我们可以对三维场景的各个组件进行优化，如顶点位置、灯光、相机投影矩阵等。借助于梯度下降算法进行值更新，并且很容易集成到神经网络中而无需额外参数，我们可以计算重投影误差以矫正相机位姿。经过实验证明，我们优化相机后生成的纹理与原始的纹理相比模糊伪影区域显著减少，接近于真实场景的外观。经过对抗生成网络后可以恢复出模型真实表面外观。经过公共数据集和我们自己拍摄的数据集的定量和定性结果比较，我们的算法比单纯用优化法和合成法，性能均有显著提高。
\section{相关工作}
近十几年纹理优化方法被进行广泛的研究，并提出了很多行之有效的算法。本文回顾和总结一些与我们工作相关的纹理优化方法。\par
\vspace*{2mm}\noindent{\bf 基于优化的方法：}生成纹理的方法有加权融合顶点颜色或者投影图像至几何模型，由于不可避免的误差因素存在加权融合算法会产生模糊伪影现象，投影方法图像会产生纹理缝隙。基于优化方法的常常会针对误差项进行优化矫正。zhou等人~\cite{zhou2014color}设计了一个新的纹理映射框架，该方法对相机位姿优化的同时对纹理图像进行 warping 操作来对几何误差造成的纹理不对齐进行校正。该方法需要对模型 进行细分，这样大大的影响了算法的效率。而且该方法的纹理结果很大程度上依赖于 网格细分的粒度。。3DLitle [48] 利用平面近似对整个场景的重建模型进行一个抽象的表示，然后他们使用平面 图元作为约束来优化相机位姿和纹理。最终他们可以得到一个平面抽象表示的带有高 保真度纹理的重建模型。然而这个方法使用平面对场景进行抽象的表示会丢掉很多几 何细节。Fu等人~\cite{fu2018texture} 提出了一种全局到局部的非刚性优化方法来调整摄像机的姿态漂移，并纠正几何误差引起的纹理坐标漂移。
\vspace*{2mm}\noindent{\bf 基于合成的方法：}最近，Bi 等人 [9] 采用基于 patch 的图像合成策略在每个视口重新 合成一张新的目标纹理图，这张新的纹理图相当于对原始纹理图进行了重组，使得所 有的生成的目标图之间能够完全对齐。该方法可以有效地解决由于相机位姿漂移和几 何误差造成的纹理图像之间不能完全对齐的问题。然而该方法的一个主要问题是它十 分耗时，效率太低。Huang等人~\cite{JingweiHuang2020AdversarialTO}使用从弱监督视图中获得的条件对抗损失为近似表面生成逼真的纹理，使用基于学习的方法训练纹理目标函数，以保持对摄像机姿态和几何畸变的鲁棒性。最近fu等人~\cite{fu2021seamless}提出新的纹理映射方法，使用一个三向相似度函数来重新合成纹理图边界条纹内的图像上下文，减少纹理接缝的出现。最后引入全局颜色协调方法来解决从不同视点捕获的纹理图像之间的颜色不一致，生成视觉逼真的纹理映射结果。Waechter 等人 [105] 在为每个面选择一个最优的纹理之后提出一个全局的颜色一 致性校正策略来减轻纹理之间的缝隙效应，该方法可以有效地减少多张纹理混合导致 纹理的模糊和重影，并有效地减轻纹理之间的缝隙。但是，该方法也没有进行任何相 机位姿和几何误差校正，所以并不能完全消除纹理之间的缝隙。


\section{算法流程}
我们的方法旨在通过RGB-D相机采集的深度图片和彩色图片，并将不同视角的彩色图适配到几何模型上以生成具高保真的纹理贴图。为了达成这一目标，我们提出结合相机位姿优化与纹理生成算法提高纹理模型的清晰度和保真度。该算法的具体流程分为四个步骤。
\noindent\textbf{模型重建：}RGB-D相机采集的原始数据为彩色图序列和深度图序列。在公共数据集上，一般会提供深度图集、彩色图集、对应的相机内外参以及用重建算法~\cite{SungjoonChoi2015RobustRO,AngelaDai2016BundleFusionRG,RichardNewcombe2011KinectFusionRD}重建出的初始几何模型。在我们自己拍摄的数据集上我们需要自己重建几何模型以及估计对应的相机位姿。我们利用重建算法~\cite{LongYang2018SurfaceRV}重建初始几何模型，并估计初始的相机位姿。\par
\noindent\textbf{预处理：}在预处理阶段，我们从所有帧中选出最清晰的彩色图像当作候选图像，消除冗余和重叠度较高的视角。同时保证所选关键帧的视角覆盖范围和原始集合一致。我们额外采用多视觉进行监督，避免单视角可能因相机飘移导致异常值影响纹理优化，我们为每一帧都选取邻近视角，同时用源视角和扭曲后的视角进行监督。\par
\noindent\textbf{相机位姿优化：}为了借用可微分渲染框架优化相机位姿，首先渲染三维模型至某一视角的彩色图像，然后再用真实图像与渲染图像做图像间的损失以优化相机投影矩阵。优化相机位姿完成后，我们将模型顶点投影至所有可见视角获取顶点颜色，再用加权平均算法融合颜色以生成纹理，为了便于后续纹理优化我们将颜色以UV纹理形式保存。\par
\noindent\textbf{纹理优化：}由图像翻译模型~\cite{isola2017image,wang2018high}得到启发，我们借助对抗生成网络在初始纹理基础上重生成新的UV纹理，我们已知纹理到图像的映射关系，并且获取相对准确的相机位姿。用源视角$I_A$的邻近视角$I_B$扭曲到源视角当作真例，渲染图像为假例，以对抗生成模型优化纹理图。\par

\subsection{数据处理}
\noindent\textbf{重建模型：}RGB-D数据我们使用消费级深度相机kinect V1或者kinect v2在固定曝光和白平衡模式下拍摄。拍摄完成后已知信息只有初始拍摄所得的为深度流和彩色流以及相机标定后的内参，并没有相机位姿与重建模型。由最近纹理优化方法G2Tex~\cite{fu2018texture}得到启发，G2Tex算法输入是深度相机扫描的深度图集合和彩色图集合，借助基于RGB-D三维重建算法~\cite{LongYang2018SurfaceRV}利用稀疏序列融合（Sparse-Sequence Fusion，SSF）策略并加以改进，获取高可用、高质量的RGB-D序列重建TSDF模型，并用移动立方体算法抽取出网格模型。重建几何模型选帧策略依照如下公式进行：
\begin{align}
E(i)= \lambda_{1} E_{\mathrm{jit}}(i)+\lambda_{2} E_{\mathrm{dif}}(i)+\lambda_{3} E_{\mathrm{vel}}(i)+\lambda_{4}E_{\mathrm{cla}}(i)+E_{\mathrm{sel}}(i)
\end{align}
其中$E_{\mathrm{sel}}(i)$当前帧$i$是否作为三维重建的有效帧的控制项，$E_{\mathrm{jit}}(i)$衡量当前帧$i$与前一个有效帧的抖动强度比较，尽量保证选取平滑非抖动帧进行重建，$E_{\mathrm{dif}}(i$确保前后有效帧间有超过阈值重叠覆盖率，避免相机跟踪失败，$E_{\mathrm{vel}}(i)$度量相机运动速度，避免相机运动过快产生的模糊帧，$E_{\mathrm{cla}}(i)$是图像清晰度度量指标，衡量彩色图片清晰度以保证选择的彩色图像最清晰。经过重建算法后我们得到初始网格模型$M$。
\noindent\textbf{关键帧获取。}由于RGB-D相机是手持进行拍摄，拍摄过程中不可避免的出现抖动相机运动过快，往往会造成所拍摄图片发生模糊、失真等现象。为了根除这个现象，我们会根据 Crete 等人提出的图片的模糊度度量指标 ~\cite{FrederiqueCrete2007TheBE}评估每一帧。对于彩色图超过100张的数据集，经验上我们设置一个尺寸为5的滑动窗口，在每个窗口内选择模糊度最小的图片作为关键帧$KF$，根据数据集的大小灵活调整窗口尺寸。经过实验证明选择关键帧并不会影响优化效果而且会线性减少优化时间。 \par
\noindent\textbf{辅助视角选取。}受Huang等人~\cite{JingweiHuang2020AdversarialTO}的启发，我们为每个原始视角$T_s$选取一个辅助视角$T_t$,使得辅助视角重投影至原视角作为真例以此来监督渲染的图片。令$p_s$，$p_t$分别表示原始视角和目标视角的齐次坐标，则二者的对应关系可以描述为:
\begin{align}
	p_s\sim KT_{t\rightarrow s}D_tK^{-1}p_t
\end{align}
其中$D_t$表示目标视角的深度值。由于遮挡和深度图存在噪声的缘故，目标视口扭曲到原始视角因未对齐而产生残缺现象。为了防止残缺部分过大，我们为任意两个视口计算z方向夹角$\theta$，当$\theta\le15^{\circ}$时两个视角才被符合源视角-目标视角位姿对儿集合。

\subsection{相机参数优化}
在传统的优化相机方案中。通常计算重投影

在基于RGB-D的三维重建中一般使用光束平差法估计初始的相机位姿。由于存在噪声相机位姿估计存在累计误差，即使有回环检测也并不都能完全消除。使用不完美的相机位姿渲染图片，渲染图片和采集图片会发生错位现象。借助于可微分渲染框架我们可以朝着期望的方向对相机位姿进行优化。\par
渲染模型选择上，我们使用标准的图像渲染流程即光栅化和布林冯着色模型。渲染目标遵从以下目标：
\begin{align}
	C_i = (I_a + I_d) * P_i + I_s
\end{align}
其中$I_a,I_d,I_s$分别表示环境光、漫反射光和高光项，$C_i,P_i$分别表示彩色图像素和纹理像素。为了最大限度的还原具有保真度的纹理，我们只保留环境光。对于相机外参T，令$T = (R,t)\in \mathrm{SE} (3),R_i \in \mathrm{SO}(3)$并且$t\in\mathbb{R}^3$。 
由于我们的框架中所有模块都是可微分的，可以端到端的进行训练。每一次渲染我们会随机选择某个视角，在该视角下生成彩色图$\tilde{I}_c$深度图$\tilde{I}_d$以及阴影图$\tilde{I}_s$即$I_c,I_d,I_s = Render(M_0|T_i)$。我们首先使用光度一致性损失来优化关键帧的相机位姿
\begin{align}
	E_c = \left \| I_c - \tilde{I}_c  \right \|_1 
\end{align}
然而仅仅具有光度一致性损失不足以约束相机位姿以朝着我们所想的方向优化，在某些场景纹理单一或者纹理较少，相机仍会发生漂移现象。因此我们额外考虑几何损失来增强几何一致性。
\begin{align}
	E_d = \left \| I_d - \tilde{I}_d  \right \|_1 
\end{align}
受SoftRas~\cite{ShichenLiu2019SoftRA}启发我们也采用轮廓损失来对几何进行约束。这个阴影损失函数为：
\begin{align}
	E_s = 1 - \frac{\left \| \hat{I_s}\otimes I_s  \right \|_1 }{\left \| \hat{I_s}\oplus  I_s- \hat{I_s}\otimes I_s \right \| }_1  
\end{align}
其中$\otimes $和$\oplus $分别是按元素计算的乘法运算符和运算符。最后我们按照每个损失对优化相机参数的贡献来赋予每个损失不同的权重。
\begin{align}
	L_T = \lambda_c E_c + \lambda_d E_d +\lambda_s E_s
\end{align}经验上我们设置$\lambda_c = 0.1$,$\lambda_d = 1$,$\lambda_s = 1$。

\subsection{结合自适应细分的网格优化}
仅仅优化相机参数不足以保证网格上任意顶点投影至每个视角得到一致颜色，尤其是在网格重建误差较大情况下。不仅如此，在重建三维模型时一般会选用加权平均方法来抵抗重建过程中的噪声，虽然这种方法卓有成效，但是会造成过平滑的效果致使网格失去几何高频细节。我们同样也借助于可微分渲染方法恢复出几何表面的高频几何细节。我们为网格上每个顶点施加偏移量来矫正几何误差。\par
仅仅只更新顶点位置不足以保证几何模型细节突出，因为几何模型本身就过于平滑。我们采用质心细分网格方法增加三角形面片数目，一方面能使得模型更加契合真是世界场景，另一方面能减小顶点优化时发生漂移现象。\par
即使如此，细分网格代价是巨大的，增加顶点面片数目会增加渲染时间，并且在一些含有平面较多且纹理单一的几何模型上细分视觉效果并不明显。我们经验地发现优化几何模型时，顶点移动频繁发生在纹理丰富的地方，而纹理较为单一时顶点移动并不明显。因此我们建议根据场景本身的纹理丰富程度来决定是否要细分的程度。具体的，我们遵循一下步骤:

\begin{enumerate}
	\item 用soble算子提取所有视角的梯度图$\nabla g_i$，作为细分面片的依据。
	\item 计算几何模型上每个面片$f_j$投影至每个可见视口$i$的面积$A_{ij}$,并求出面积总和$\sum_{i}^{n} A_{ij}$。
	\item 计算每个面片$f_j$采样概率$\sum_{i}^{n} A_{j} / \sum_{j}^{m}\sum_{i}^{n} A_{ij}$，其中m为几何模型中面片数量，n为视角数量。
	\item 按照概率对所有面片进行无放回随机抽样，并设定采样概率阈值，在阈值概率之上面片才会被选取。
	\item 对所选择面片$f_j$进行质心细分。
\end{enumerate}

我们在第四步随机抽样时，为了防止太多的面片不在查询集合中，我们使用中位数而不是平均值作为采样阈值。细分完成后，我们仍用同样的方式增加对应的纹理坐标，以保持顶点和纹理坐标的对应关系。\par
为了防止优化过程出现异常情况，如裂缝现象。我们先用meshlab~\cite{LocalChapterEvents:ItalChap:ItalianChapConf2008:129-136}剔除重复顶点和面片以及零面积面片,以保证几何模型的规范性。优化几何模型时仅仅有图像间损失是远远不够的，因为每次迭代中顶点移动不受约束，会破坏网格模型的拓扑结构，导致退化的三角形。因此我们在图像损失基础上增加了几何正则化项，拉普拉斯项、法线一致性项和$L2$项。\par
拉普拉斯项定义为顶点坐标减去其临近顶点的加权和，在优化过程中可以保持局部几何特征不变。在优化过程中，拉普拉斯损失项可以帮助模型收敛到一个更加平滑的解，从而提高几何模型优化的准确度和稳定性。它也可以有效抑制因噪声或不规则采样而产生的噪点，使得优化结果更加符合实际。令$V \in \mathbb{R}^{n\times3}$为存储顶点位置的矩阵，则拉普拉斯项定义为：
\begin{align}
	E_l = \sum_{i=1}^{n}\left \| LV \right \|_2 
\end{align}
其中$L\in\mathbb{R}^{n\times n} $是网格的拉普拉斯图。\par
在优化过程中为了防止顶点偏移太远我们用$L_2$正则化项来约束顶点偏移量。其中$\mathbf{v}_{i}$为初始网格的顶点坐标，$\widetilde{\mathbf{v}}_{i}$为当前网格顶点的坐标。
\begin{align}
	E_{r}=\sum_{i}^{n}\left\|\mathbf{v}_{i}-\widetilde{\mathbf{v}}_{i}\right\|^{2}
\end{align}
最后，为了保证网格具有平滑性，我们额外使用了网格法线一致性损失。即利用余弦相似度计算相邻面片的法线一致性。
\begin{align}
	E_n= \frac{1}{|\overline{\mathcal{F}}|} \sum_{(i, j) \in \overline{\mathcal{F}}}\left(1-\mathbf{n}_{i} \cdot \mathbf{n}_{j}\right)^{2}
\end{align}
其中$\overline{\mathcal{F}}$代表共享一条边的相邻三角形面片的集合。$i$,$j$表示任意一对儿三角形面片例如$f_i$,$f_j$的法线。\par
最终网格重建损失项表示如下：
\begin{align}
	L_M = L_T + \lambda_l E_l +\lambda_r E_r+\lambda_n E_n
\end{align}
在实际优化中我们分别设置$\lambda_l = 1000$，$\lambda_r = 1000$,$\lambda_n = 10$。
\subsection{纹理重合成}
仅仅只矫正相机位姿和网格顶点位置是不够的，优化结果并不能完全消除噪声。况且我们采用顶点加权融合方式生成纹理，仍存在模糊并且缺失细节。由Huang等人~\cite{JingweiHuang2020AdversarialTO}得到启发我们用对抗生成网络学习重建模型和拍摄的彩色图片错位的容忍度。我们使用基于$\pi$GAN\cite{chanmonteiro2020pi-GAN}的网络用像素重生成管线合成纹理图。我们使用辅助视角重投影至原视角$I_c^{B\to A}$为真例监督可微分渲染生成的彩色图假例以更新纹理。相应地对抗损失函数定义如下：
\begin{align}
	L_{a d v}=\log D\left(\boldsymbol{I}_{A}, \boldsymbol{I}_{B \rightarrow A}\right)+\log \left(1-D\left(\boldsymbol{I}_{A}, \boldsymbol{I}_{A}^{D R, t}\right)\right) 
\end{align}
经过若干次迭代交替训练判别器$D$和纹理$P$后，判别器会识别渲染图片中存在的伪影模糊或者裂缝，纹理会重新生成像素以弥补渲染图片和真实图片之间错位现象，使得两者充分接近以愚弄判别器。经过优化后的纹理相比与用加权融合方法生成的纹理更加真实，更接近于真实拍摄的彩色图片。
\subsection{交替优化}
相似于之前工作，我们使用联合优化策略优化相机参数、几何模型和纹理。我们用相机-网格-纹理优化顺序进行。一方面是在相机和网格优化中纹理生成方法不同于神经网络的像素重生成方法，另一方面在矫正相机参数和网格后，对抗生成网络效果重生成的纹理更加贴近于真实世界场景。具体地我们采用外部循环方法分别优化参数集$(T,M,P)$。我们首先固定参数$(M,P)$，通过最小化损失函数$L_T$以优化每一帧的相机参数$T$至$T'$；其次，我们使用参数$(T',P)$最小化损失函数$L_M$优化几何模型$M$至$M'$；最后我们使用并固定前两次的结果参数$(T',M')$最小化对抗损失$L_{adv}$来优化纹理$P$至$P'$。\par
在时间和效率的权衡下我们重复外循环优化策略3次。并且我们遵从由粗到细的策略优化不同的目标。具体地，在每一次外部迭代$t\in \left \{ 1,2,3 \right \}$中我们用指数衰减方式控制内部迭代次数，即内部迭代次数为$\text{epoch}  =\frac{s}{2^{t-1}}$，其中s为每一个内部迭代初始的轮数。除此之外，学习率也会相应地进行减半操作。我们的经验发现，利用指数衰减测率，可以保证最终效果的同时显著减小优化时间。我们设置初始的优化相机参数、优化几何模型和纹理次数为50，50，100。

\section{实验}
\paragraph*{比较方法}
在这个部分我们会和最新和方法G2Tex~\cite{fu2018texture},JointTG~\cite{YanpingFu2020JointTA},ATO~\cite{JingweiHuang2020AdversarialTO},Intrinsic3d~\cite{RobertMaier2017Intrinsic3DH3}在公开的RGB-D数据集上并用他们在GitHub提供的源代码在进行比较。因为ADJOIN~\cite{9705143}没有公开代码可得所以不进行比较。然后在我们自己拍摄的RGB-D的数据集上进行比较，最后我们会展示消融研究以证明我们联合优化框架的有效性。我们整个实验结果都在单个NVIDIA GeForce RTX3090 24GB服务器上计算得出。


\paragraph*{评估指标}
为了对所生成的纹理进行定量的研究，我们采用多种指标来衡量纹理和真实图片之间的差异。因为没有准确的指标来直接衡量生成的纹理图集，所以我们仍旧将纹理渲染为图像，然后利用经典的图像的评估指标，如峰值信噪比（PNSR）、结构相似性（SSIM）和感知相似度（LPIPS）来侧面衡量纹理清晰度和保真度。注意，我们使用所有视角评估的平均值作为最后的结果。
\subsection{在公共数据集上进行评估}
我们首先在ATO~\cite{JingweiHuang2020AdversarialTO}发布的椅子数据集上数据集上数据集上进行实验比较。如图X所示，可以看出我们的方法得到了最佳的效果，ATO采用对抗神经网络方法合成纹理图，虽然能够容忍较小的相机误差和几何误差，但是在局部地方仍然有模糊现象。G2Tex采用面投影方法，并优化了相机位姿，能够产生的纹理整体比较清晰，但是局部地方会有明显裂缝出现，这一点纹理比较丰富的数据集上更加明显。我们额外进行了定量评估，生成模型后我们在所有视角下评估定量指标最后取平均值展示。如表所示，我们的结果在椅子数据集上的定量指标（如上所示的）均超过了，最新的纹理优化方法。\par
其次我们在Intrinsic3D~\cite{RobertMaier2017Intrinsic3DH3}发布的数据集上进行评估,我们和intrinsic3D方法和JointTG方法，进行实验对比。这两个方法都分别实现了一个联合优化框架，同时优化纹理和几何模型。如图XX所示，我们的方法在纹理优化和几何优化上都取得了更佳的效果。由于Intrinsic3D基于阴影恢复形状（SFS），恢复几何细节方面效果非常好，但是由于SFS固有的缺陷，intrinsic3d很容易出现纹理复制伪影，尤其是当视角非常稀疏的情况。JoinTG的看起来视觉效果非常好，但是在几何和纹理细节方面，我们的结果比JoinTG更好，尤其是在人物手部眼睛方面，我们的方法恢复的细节更多，看起来更接近于拍摄图片。\par
最后，我们比较各种基线方法在zhou等人~\cite{Zhou2018}发布的Fountain数据集，该数据集视角非常稀疏。在此数据集上，我们的方法可以很好的处理标准化数据集，如图所示。我们的方法在几何细节和纹理细节方面对比其他方法展示出优越性。如图红色框中所示，我们的方法能恢复出清晰的喷泉模型文字，产生一致的纹理。其他方法在局部区域要么会产生模糊伪影要么会产生不一致的边界如G2Tex。我们额外实现了定量评估，如表所示，我们的方法对重建误差进行矫正，再用像素重生成管线合成纹理图，对几何误差和相机位姿误差更具有鲁棒性。经过联合优化后重建出的纹理，渲染的图片更接近于真值。\par
\subsection{在我们自己拍摄的数据上进行评估}
\subsection{消融研究}
在这个部分，我们将研究方法中各个部分的有效性。为了更有说服力，我们选择真实场景中的数据集Fountain。首先我们移除各个部分，没有优化相机位姿，没有优化几何模型，没有对抗生成网络，不采用自适应细分的情况，最后我们还讨论自适应细分方法，质心细分和基于边的细分效果。\par
如图所示，我们展示了移除的各个部分效果。没有矫正相机位姿情况下，渲染图片和真实图片会有错位现象，借助于对抗生成我们仍都能得到清晰的纹理，但是缺乏细节。没有几何细化，看起来对纹理生成效果最小，但是会在细节处产生轻微的伪影。没有对抗生成网络时，无论是纹理细节还是整体视觉效果都明显下降，并在定量评估中我们同样得出了一致的结论。\par
我们采用自适应细分方法，在纹理丰富处产生更多的顶点，保证视觉效果的同时减少数据量。如图所示，我们分别基于面和基于边的做法，虽然基于边的细分能产生更加均匀的效果，但是会导致未细分面片和已细分面片的公共边上的顶点产生读书不平衡，而基于面的做法保持规整。当优化几何模型时，基于边的细分容易导致拓扑结构的破坏，即三角形变形为四边形从而产生奇怪的几何模型。
\noindent \textbf{运行时间}
我们的优化时间随着数据集帧数线性增长，我们的方法在ATO发布的数据集上每个场景花费平均63分钟，其中优化相机位姿、几何、纹理时间占比分别为30，40,30。
