% !TeX root = ../Template.tex
% 本LaTeX模板的一般使用说明
%\chapter{绪论}
\chapter{三维重建技术介绍}
\section{数据获取}

深度相机是一种能够获取三维场景信息的相机。它通常使用激光或者红外干涉仪来测量距离，并且能够根据这些距离信息生成三维点云或者深度图像。本文采用微软发布的Kinect深度相机，以获取三维场景的深度信息和彩色信息。我们实验数据集主要采用Kinect v1相机采集的深度图像和彩色图像，它采用结构光方法进行深度测量，通过投影仪发射伪随机散斑红外光点，然后拍摄被测物体采集结构光图像，获取物体表面深度。深度相机测量范围在0.5m ~ 4.5m范围并且随着距离的增加测量精度会下降，kinect v1相机分辨率只有（640 $\times$ 480）,彩色图的低分辨率使得拍摄图片更易模糊而且该结构化原理相机易受强光源的干扰，由于不完美的硬件设施所以在三维重建中往往在算法层面设法抵抗深度采集的噪声。Kinect v2相机在第一代基础上做了改进，第二代依据TOF（Time of Flight）飞行时间来测量深度值，通过给目标发射连续的光脉冲，经过目标反射，通过传感器接受回来的光，记录飞行的时间，计算出到目标的距离。Kinect v2 可以拍摄彩色图像分辨率为（1920$\times$1080）深度图像分辨率为（512$\times$424），测量深度值更加准确。得到采集深度数据往往转化为单通道无符号16位数据图片中，单位为毫米。\par 
深度图上的像素和三维模型上的顶点存在着一一对应关系。三维世界中为了计算方便通常分为相机坐标系、世界坐标系、图像坐标系、像素坐标系以及对象坐标。三维模型顶点变换到图像像素，正是基于小孔成像原理。为了模型投影，首先将处于世界坐标系内的三维顶点利用刚体变换至相机坐标系中，然后根据透视投影至图像坐标系中，为了得到相同尺度下的统一表示，需要数字化处理至像素坐标系，最后经过图像畸变矫正得到真实图片。不失一般性，设三维模型上一点$\boldsymbol{p} = (X_w,Y_w,Z_w)^\top $，定义刚体变换为$\mathbf{g}\left(\boldsymbol{p}, \mathcal{T}\right)=\mathcal{T} \boldsymbol{p} = \boldsymbol{R} \boldsymbol{p}+\boldsymbol{t}$ ,
其中$\mathcal{T}=\left(\boldsymbol{R}, \boldsymbol{t}\right) \in \mathrm{SE} (3), \boldsymbol{R}   \in \mathrm{SO}(3) \text { 和 } \boldsymbol{t} \in \mathbb{R}^{3}$。得到相机坐标系中一点$\boldsymbol{p'}=(X_c,Y_c,Z_c)^\top$。设$\mathrm{u}$为在图像平面$\mathrm{I}$上的投影为:
\begin{align}
 \mathbf{u}\left(X_c, Y_c, Z_c\right)=\left(\frac{X_{c} f_{x}}{Z_{c}}+c_{x}, \frac{Y_{c} f_{y}}{Z_{c}}+c_{y}\right)^{\top}
\end{align}
其中$f_x$和$f_y$是焦距长度，$\left(c_{x}, c_{y}\right)^{\top}$是光心位置。最终得到二维像素位置$\boldsymbol{x}=(u, v)^{\top}$。设相机内参为$K$则映射关系可以描述为
\begin{align}
\left(\begin{array}{c}
u \\
v \\
1
\end{array}\right)=\frac{1}{Z}\left(\begin{array}{ccc}
f_{x} & 0 & c_{x} \\
0 & f_{y} & c_{y} \\
0 & 0 & 1
\end{array}\right)\left[\begin{array}{cc}
\boldsymbol{R} & \boldsymbol{t} \\
\mathbf{0}^{T} & 1
\end{array}\right]\left(\begin{array}{c}
X_{w} \\
Y_{w} \\
Z_{w} \\
1
\end{array}\right)=\frac{1}{Z} \boldsymbol{K} \mathcal{T}\left(\begin{array}{c}
X_{w} \\
Y_{w} \\
Z_{w} \\
1
\end{array}\right)
\end{align}
由于相机制造工艺不同，相机采集图片过程中会发生失真现象称为相机透镜畸变，因为透镜不能完全遵从针孔模型的假设。畸变主要分为径向畸变和切向畸变，光线在原理透镜中心的地方比靠近中心的地方更加弯曲。因此径向畸变从中心开始，越靠近边缘畸变程度越高，可以分为桶形和枕形畸变。切向畸变由于透镜和CMOS的安装位置误差使得透镜本身与图像平面不平行，属于制造工艺的缺陷。对于径向畸变可以通过泰勒级数进行矫正。
\begin{align}
\begin{array}{l}
x_{\text {corrected }}=x\left(1+\mathrm{k}_{1} \mathrm{r}^{2}+\mathrm{k}_{2} \mathrm{r}^{4}+\mathrm{k}_{3} \mathrm{r}^{6}\right) \\
y_{\text {corrected }}=y\left(1+\mathrm{k}_{1} \mathrm{r}^{2}+\mathrm{k}_{2} r^{4}+\mathrm{k}_{3} \mathrm{r}^{6}\right)
\end{array}
\end{align}
其中$x,y$为经过透视投影后的图像位置坐标，$r$为坐标点距成像中心的距离。而切向畸变可以通过如下公式矫正：
\begin{align}
\begin{array}{l}
x_{\text {corrected }}=x+\left[2 p_{1} y+p_{2}\left(r^{2}+2 x^{2}\right)\right] \\
y_{\text {corrected }}=y+\left[2 p_{2} x+p_{1}\left(r^{2}+2 y^{2}\right)\right]
\end{array}
\end{align}
由上述公式共有$K_1,k_2,k_3,p_1,p_2$五个参数，求解参数过程就是图像去畸变过程，具体原理可参考相机标定原理，如经典的棋盘格标定~\cite{888718}。

\section{表面表示}
纹理为三维模型的视觉外观具象化的表示方式，依赖于三维模型存在。三维模型有多种表达方式，具体选用何种表面表示，需要考虑重建算法和应用场景。三位维模型表示可分为显示表示包括点云、网格、体素等，隐式表示为占据网格、符号距离场（Signed Distance Function，SDF）等。\par
在SLAM重建过程中为了满足实时重建要求，一般采用隐式表示方法，截断符号距离场（Truncated Signed Distance Function，TSDF），另一个是面素（surfels）。为了建模真实场景，首先选定建模的三维空间，通常为预定义一个立方体块，例如$7m \times 7m \times 7m$,再将三维空间切割为多个小块，分辨率通常为$128 \times 128 \times 128$，其中每个小块被称为一个体素。原始的体素表示不适用于大场景物体的重建，因为随着分辨率的提升计算代价和内存代价呈指数级提升，多用于小场景的重建~\cite{nguyen2018rendernet}。由于体素很容易进行数据更新和融合，并且易用于GPU并行化处理，所以在基于RGB-D的三维重建中多用体素表示法，但是由于内存限制往往会采用TSDF模型表示。TSDF模型中存储每个体素存储该小块与其表面最近物体表面的距离，若是体素小块在物体前面就存储正距离，位于物体表面后就存储负距离。\par
TSDF算法~\cite{BrianCurless1996AVM}分为三个步骤：第一，需要对整个三维场景划分为$N$个规则的网格，以表示整个场景。TSDF中每个网格存储距离值$D(p)$和相关权重$W(p)$，其中$p$为体素空间中的一个网格。第二，计算当前深度帧$\mathcal{Z}_{i}$上的TSDF值$d_i(p)$和权重$w_i(p)$，设$p$为单个体素在世界坐标系上的位置$p = (X_w,Y_w,Z_w)^\top$,根据上述介绍的投影流程获取在相机坐标系下z分量,以及投影至深度帧上的像素所代表的深度值。则$d_i(p)$可表示为：
\begin{align}
d_{i}(\boldsymbol{p})=\Psi\left(\left(\mathcal{T}_{i} \boldsymbol{p}\right)_{z}-\mathcal{Z}_{i}\left(\pi\left(\mathcal{T}_{i} \boldsymbol{p}\right)\right)\right) 
\end{align}
其中$\Psi(x) =\max \left(-1, \min \left(1, x\right)\right)$和投影函数$ \pi: \mathbb{R}^{3} \mapsto \mathbb{R}^{2}$。当前帧权重可得：
\begin{align}
w_i(p) = \frac{ \cos \theta} { \left(\mathcal{T}_{i} p\right)_{z}-\mathcal{Z}_{i}\left(\pi\left(\mathcal{T}_{i}p\right)\right)} 
\end{align}其中$\theta$为$p$点的投影光线与物体表面法向量的夹角。
第三，当前帧和全局模型结果进行融合。现有的方法普遍采用的加权平均的方法把所有的 TSDF 值融合到全局模型中，即当前帧的体素依赖于上一帧，整个体素的TSDF值为所有帧的权重乘以得到的TSDF再平均。体素更新公式如下：
\begin{align}
D_{i}(p)=\frac{W_{i-1}(p) D_{i-1}(p)+w_{i}(p) d_{i}(p)}{W_{i-1}(p)+w_{i}(p)} 
\end{align}

\begin{align} 
W_{i}(p)=W_{i-1}(p)+w_{i}(p)
\end{align}
其中$D_i(p)$表示融合第$i$帧后的体素加权融合结果，$W_i(p)$代表相关权重。融合所有帧后可以得到全局模型，之后我们可以用光线投射（ray casting）算法~\cite{511}和移动立方体（Marching cubes）算法~\cite{lorensen1987marching} 抽取三维网格模型。在基于RGB-D相机的静态三维重建中ElasticFusion~\cite{ThomasWhelan2015ElasticFusionDS}使用面素（Surfel）表示法。每个Surfel代表一个小的面片，包含面片位置、面片方向、颜色、权重、半径
时间戳等信息。面素的大小和密度由Surfels表面曲率决定，采用Surfels表示不需要考虑重建模型表面的拓扑结构，所以通过对面素的增加删除和移动操作对表面进行修改，所以ElasticFusion用Surfels表示法优化重建土方是提高重建和位姿估计大的精度。\par
除了SLAM中的两种表面表示法，基于学习的三维重建中也使用点云、三维网格、体素的隐式表示方法进行非实时场景重建。例如Lars Mescheder等人~\cite{LarsMescheder2018OccupancyNL}提出占据网格，每个体素包含一个二进制占据状态（占据/未占据）隐含地将3D表面表示未深度神经网络分类器的连续决策边界，理论上可以表达三维模型无限制的分辨率，而不需要过多的内存占用，有效解决了原始体素表示存在的高内存低分辨率的缺陷。同样的以上介绍的SDF表示方式也很契合多层感知机（Multi-layer Perceptron，MLP），将二维决策边界作为等值面进行表示，这种表示不需要占用很多内存也不需要很深的网络模型就可以表示三维场景。因此NeRF~\cite{mildenhall2021nerf}，deepSDF~\cite{park2019deepsdf}等可以以低成本代价获取高质量的重建模型。而且基于SDF表示法易于与光线追踪算法~\cite{hart1996sphere}结合，从而获取高保真的重建模型。点云作为原始的表面表示法，使用点集表达三维场景，好处是很容易利用传感器进行获取，并只需关注点集的三维坐标。基于点云的表达方法PointNet~\cite{qi2017pointnet}和PointNet++~\cite{qi2017pointnet++}，使用最大池操作来提取全局形状特征，并且该技术被广泛用作点生成网络的编码器，完成深度学习的分类分割任务。然而，基于点云的表示法缺乏拓扑信息，因此使生成三维网格变得不那么容易，往往使用泊松重建算法~\cite{kazhdan2006poisson}后处理生成三维网格，因此不适用于产生水密表面。基于三维网格表示，可以表达三维模型的表面拓扑结构，并且无需关注三维空间内部，因此比较灵活而且相比体素表示更节省内存。相比于点云，网格表示法不仅考虑点的三维空间坐标也关注点之间的连接关系，适用于产生水密表面。由于网格表示更容易进行存储，编辑，而且有较多的相关开源软件，如meshlab~\cite{LocalChapterEvents:ItalChap:ItalianChapConf2008:129-136}进行操作，因此网格表示在实际应用中最广泛。但是基于网格的三维重建算法并不那么通用，因为大多数方法会产生自相交的网格，而且只能生成简单拓扑结构的物体，并且需要来自同一物体类别的网格模板。究其原因，网格在发生形变过程会改变顶点之间的连接关系，拓扑结构自然而然地发生改变。重建过程中无法预测应该重建顶点的数目，也无法准确预测顶点之间的连接关系，所以只能通过预设置固定拓扑的模板网格来作为初始的重建模型。最后，网格模型拓扑结构复杂多变，不像二维图像一样规则，无法像隐式表示方法一样契合神经网络，因此基于学习的重建方法，只适合重建拓扑结构简单而且较小的物体。网格表面表示常用三角形或者四边形来表示每个面片，其中三角形面片最为常见。\par
因为网格表示方法更加契合纹理图集而且易于进行纹理编辑，所以本文选择网格表示作为几何模型，经过SLAM重建完成后，我们会用Marching cubes算法抽取三维网格。由于在重建中由于遮挡，噪声的存在重建出的网格往往是非水密表面，残缺较严重，不符合2D流形规则。所以给纹理优化步骤带来了新的问题与挑战。
\section{纹理获取与表示}
\subsection{纹理表示}
纹理作为三维模型的外观表示，与三维模型关系密不可分。为了便于编辑纹理通常用单独的方式存储纹理，同时保留三维模型与纹理的映射关系。通俗来讲，纹理是一种存储每个顶点的颜色的信息，纹理不仅包含像素颜色，也维持各个像素位置与顶点的对应关系。艺术家纹理喜欢适用于纹理来增加三维模型有趣的外观，即使模型本身并不复杂，有了纹理可以伪造几何细节使得看上去更加丰富。在游戏中常常使用纹理来展示丰富的场景，同时不增加计算代价，如海浪贴图，墙壁贴图等。纹理一般以2D图片形式存在（纹理可以进一步分为一维、二维和三维纹理，但是本文只关注二维纹理，其它的不做过多介绍）。相比于彩色图像，纹理反应了图像的本质特征并不依赖于图像颜色或者亮度变化，刻画了图形区域的像素灰度级空间分布属性。纹理重建/优化中通常使用以下表示方式。\par
\vspace*{2mm}\noindent{\bf 顶点纹理：}纹理作为顶点的颜色表示，最简单的方法是在存储三维模型时直接把对应顶点的颜色也存储在内。这种表示方法非常适用于包含纹理网格细分的纹理优化算法，例如Colormap~\cite{zhou2014color}和Intrinsic3D算法~\cite{RobertMaier2017Intrinsic3DH3}。但是，为了保证图形的真实度，就必须有足够多的顶点来指定足够多的颜色。这会加大存储开销，而且不易转化为其他纹理表示方式。\par
\vspace*{2mm}\noindent{\bf UV纹理：}这种表示方法额外使用彩色图像保存纹理，并且用mtl文件来存储所有纹理图像路径和材质信息。在纹理优化算法中常常使用这种表示方法。相比于顶点纹理，可以额外指定材质参数，如环境光、高光、漫反射光系数，滤光透射率等。并且由于不必额外存储顶点颜色信息，所以模型所占内存会减小。但是，顶点和纹理图像素位置关系依赖于纹理坐标$s$。以模型的存储格式obj为例，文件会存储顶点的空间坐标，顶点间的连接关系以及每个顶点对应的纹理坐标。设模型上一点$p$对应的纹理坐标为$s=(u,v)$，其中$u,v\in(0,1)$。设纹理图的长宽分别为$h\text{和}w$,那么$p$对应的像素位置为$(h\times u,w\times v)$。在重建几何模型时并未考虑纹理坐标，只能通过特定纹理块生成算法，设定合适的纹理坐标位置。改变纹理图像必须改变纹理坐标位置，这限制了应用范围。\par
\vspace*{2mm}\noindent{\bf 图集纹理：}这种表示方法比较特殊，来自于~\cite{ShichenLiu2019SoftRA}。和UV纹理相比模型的纹理图不再表示整个三维模型，而是预定义多个正方形纹理块，每个纹理块表示模型中某个三角形的面片纹理。这种表示方法适用于shapenet
~\cite{shapenet2015}数据集因为某些模型的部分面片只有空白纹理。并且和UV纹理可以方便地进行转换。\par
\vspace*{2mm}\noindent{\bf 立方体纹理：}以立方体的六个面表示场景的纹理。一种纹理映射方法是使用球面UV进行纹理映射，使用cubemaps~\cite{greene1986environment}来可视化球面域。立体图由一个单位盒子的六个面组成，记录了从一个单位球体投射出来的所有颜色信息），它被广泛用于图形学的球形映射。另一种使球形函数可视化的标准方法是使用等角图（equirectangle map）。相比于等角图，使用立方体地图，失真较少，能避免等角地图的顶部和底部的失真区域。\par
\subsection{纹理获取}
在得到每帧彩色图像的相机位姿后，我们可以将模型上所有顶点反投影至可见的所有彩色图像中，得到该张图片的颜色信息，然后通过加权平均每个顶点投影的所有颜色信息，作为该顶点的最终颜色，这样就可以获得每个顶点的颜色，最后再存储至纹理图中得到模型的初始纹理。本文使用三维网格模型，并且已经预设好纹理坐标。具体生成纹理图的算法流程如下：\par

% \begin{enumerate}[label=(\arabic*)]
% \item ab
% \item cd
% \item efg
% \end{enumerate}



\begin{enumerate}[label=(\arabic*)]
    \item 预定义一张空白纹理图片$P$，$1024 \times 1024$大小。
    \item 设纹理坐标一点$U = (u,v,1)^\top$，将纹理坐标$u$利用相机外参$\mathcal{T}$与自定义内参$\mathcal{K}$投影至纹理图$P$上，得到像素点$X=(x,y)$。其中$X = U\mathcal{T}\mathcal{K}$。
    \item 记录覆盖像素点$X$的三角形面片$F$,记录面片索引$i$，计算像素$X$在图像平面上投影三角形的重心坐标$b_X\in(0,1)$坐标范围为0~1之间。
    \item 根据像素重心坐标$b_X$和面片索引$i$,计算出像素对应的顶点$v$和法线$\vec{n}$。
    \item 利用上一步求得的顶点$v$和面片$F$,向不同视角投影，求出顶点在视角$j$上投影点的像素颜色$c_j$，然后加权平均后获取最终颜色值$c= \frac{1}{N}  \sum_j^N c_j $，N为视角总数。
    \item 将所有顶点颜色存储到空白纹理图$P$上，将图片路径存储至mtl文件中。
\end{enumerate}


% 1.预定义一张空白纹理图片$P$，$1024 \times 1024$大小。\par
% 2.设纹理坐标一点$U = (u,v,1)^\top$，将纹理坐标$u$利用相机外参$\mathcal{T}$与自定义内参$\mathcal{K}$投影至纹理图$P$上，得到像素点$X=(x,y)$。其中$X = U\mathcal{T}\mathcal{K}$。\par
% 3.记录覆盖像素点$X$的三角形面片$F$,记录面片索引$i$，计算像素$X$在图像平面上投影三角形的重心坐标$b_X\in(0,1)$坐标范围为0~1之间。\par
% 4.根据像素重心坐标$b_X$和面片索引$i$,计算出像素对应的顶点$v$和法线$\vec{n}$。\par
% 5.利用上一步求得的顶点$v$和面片$F$,向不同视角投影，求出顶点在视角$j$上投影点的像素颜色$c_j$，然后加权平均后获取最终颜色值$c= \frac{1}{N}  \sum_j^N c_j $，N为视角总数。\par
% 6.将所有顶点颜色存储到空白纹理图$P$上，将图片路径存储至mtl文件中。\par
重复上述步骤直到所有纹理坐标都投影完毕。


\section{可微分渲染技术介绍}
渲染来自于计算机图形学，按照渲染方法可以细分为光栅化、光线追踪、体渲染等。光栅化渲染算法对每一个三角形面片进行单独渲染，可以在屏幕上快速显示3D图像，但是它不能处理高级的光照和阴影效果；光线追踪算法是一种物理模拟方法。它模拟光线在3D空间中的运动，并追踪它们如何照射物体并反射或折射。这种方法能够生成高质量的3D图像，并且能够模拟各种光照和阴影效果，但是它需要较长的计算时间，常用于真实感图形渲染。体积渲染也是计算机图形学中的一种技术，用于在三维场景中模拟物体的材质和光线。它通过在每个像素上模拟光线与场景中的对象的交互来生成图像。与其它渲染技术不同，体积渲染可以模拟物体内部的材质和光线，并且可以生成逼真的阴影和渐变效果。常用的体积渲染技术有雾化、半透明渲染和烟雾效果等。由于渲染过程并不可微分，无法直接嵌入深度学习框架中，要么改变前向渲染过程~\cite{ShichenLiu2019SoftRA}使其可微分，要么手工设计后向传播的梯度~\cite{HiroharuKato2017Neural3M}。神经渲染是一个新兴的快速发展的领域，它将生成式机器学习技术与计算机图形学中的物理知识相结合，例如，通过将可微分渲染集成到网络训练中。以神经网络来代替传统的渲染器，将经典的计算机图形技术与深度生成模型相结合的方法，以获得可控和逼真的输出。目前在三维重建\cite{yariv2020multiview}或者新视角合成~\cite{mildenhall2021nerf}~\cite{zhang2020nerf++}领域已经取得了显著效果。更多关于神经渲染的介绍请参阅论文~\cite{tewari2020state}。\par
可微分渲染(DR)构成了一系列技术，通过获得渲染过程的有用梯度来解决端到端优化的集成问题。通过区分渲染，DR弥合了2D和3D处理方法之间的差距，允许神经网络在操作2D投影的同时优化3D实体而无需收集三维实体的属性或者标注。具体地，利用渲染器，相机内外参以及网格模型渲染出彩色图片，然后再和真实采集的图片进行损失函数计算得到损失，并且回传期望的梯度更新场景的各个参数，几何、材质、场景灯光和相机属性等。现有的可微分渲染研究可以分为两类：基于物理的渲染方法，专注于生成逼真的图像质量或者追求更高的性能，往往使用渲染方程~\cite{kajiya1986rendering}的微分近似来模拟真实世界的光的反射与材质属性，渲染模型更加符合真实世界光照。第二种是以性能为导向的渲染，使用简单的着色模型，如Blinn-Phong反射模型来对真实世界光照做经验上的近似。可微分渲染器通过从投影像素到3D参数生成导数来近似梯度，相比基于物理的渲染，参数量少而且效率高。\par
本文借助可微分渲染更新优化纹理、网格模型和相机，并不专注于材质、灯光等属性。权衡性能于效率，本文使用基于光栅化的渲染器pytorch3d~\cite{ravi2020pytorch3d}。渲染器包含两个部分，一个是光栅化组件：选择影响每个像素的三角形面，另一个是渲染组件：计算每个像素的颜色。光栅化算法：使用刚体变换与透视投影将三角形面片投影至图像平面上，得到像素被覆盖的若干个三角形面片，在传统的光栅化中每个像素只受其沿z轴最近的面影响。在可微分渲染器中将分配给以上三角形序列不同的概率，面片中覆盖像素位置的顶点的z坐标，离相机光心距离越近分配概率越大，最后并依据概率加权融合不同面片的形成的像素值作为最终的像素颜色。渲染算法：Blinn-Phong光照模型中物体颜色来源于三种光线的累积，分别为环境光$I_a$、漫反射光$I_d$和高光$I_s$。光照项乘以相关系数最终得出像素颜色。
\begin{align}
    L = k_aI_a + k_dI_d+k_sI_s
\end{align}
其中$k_a,k_d,k_s$分别表示环境光、漫反射光、高光系数。

\section{本章小结}
本节主要介绍了基于RGB-D相机的三维重建与纹理优化相关知识，包括数据数据获取、表面表示、纹理获取与表示和可微分渲染技术。三维重建利用RGB-D相机采集的彩色图和深度图，本文首先介绍了深度相机的特点，借着阐述相机成像模型，并推导三维模型顶点从世界坐标系向像素坐标系转换流程，最后介绍了图像去畸变原理。第二部分中，本文首先介绍三维重建模型的不同表面方式，本文讨论了三维各种表示方法，以及在不同重建场景下选择的表面方式，最后简要介绍不同表面方式的优缺点。第三部分，本文首先介绍纹理的特点以及与三维模型的映射关系，并列举了在纹理优化中使用的纹理表示方式，最后简要介绍本文为三维模型生成初始纹理模型的算法流程。最后一部分介绍了可微分渲染原理，渲染作为计算机图形学的重要内容，本文介绍了三种常见渲染方法的特点及应用场景。随后，阐述可微分渲染原理以及现阶段可微分渲染发展中的两个分支以及各自的特点。最后介绍本文工作使用的可微分渲染框架。



对于SLAM中使用隐式表示TSDF表示和Sursels表示，在基于学习的三维重建中使用现实
双目立体匹配相关知识，主要包括双目视觉成像原理和当下主流的端到端立体匹配研究技术