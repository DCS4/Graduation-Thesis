
\chapter{基于RGB-D三维重建几何模型与纹理优化}
\section{本章内容简介}

由于三维重建过程中存在着大量的噪声以及量化误差和估计误差等因素导致三维重建模型无法如真实场景般精确，正是因为这些误差存在使得纹理图像无法与模型对齐，造成不可避免的模糊伪影或者裂缝现象。直观地，要想得到高保真纹理模型必须消除或者矫正这些误差项，但是现有的方法只将注意力放在纹理图像上，生成纹理图像以适配几何模型。忽略了几何与纹理是一个整体，单纯的优化其中一项对于纹理提升有限并使得另一项成为制约获取理想纹理模型的瓶颈。本文在基于可微分渲染纹理优化（第三章所述方法）基础上提出结合自适应细分的联合优化框架，并对纹理、相机位姿、几何模型分别进行优化，然后我们又提出联合优化方案使得更快的达成优化目标，避免共同优化多个变量陷入局部最小值无法收敛，最终我们的方案能够生成精细的几何模型，准确的相机位姿，清晰的纹理。相比于传统的联合方案，我们的方法具有可扩展性和鲁棒性，能够胜任各种场景环境，如存在灯光、弱纹理条件等。实验证明我们在灯光干扰，公共数据集和我们收集的数据集上均取得良好的表现证明了我们方法的有效性。

\section{引言}


得益于消费机RGB-D相机，如Microsoft Kinect、Intel RealSense或Google Tango等商用RGB-D传感器被广泛使用，3D场景的重建得到了极大的关注人们能够很容易的对真实世界的场景进行建模得到场景和物体的三维模型，基于RGB-D相机的静态场景~\cite{RichardNewcombe2011KinectFusionRD,ThomasWhelan2012KintinuousSE,ThomasWhelan2015ElasticFusionDS,SungjoonChoi2015RobustRO,VictorAdrianPrisacariu2017InfiniTAMVA,AngelaDai2016BundleFusionRG}和动态场景重建~\cite{newcombe2015dynamicfusion,innmann2016volumedeform,yu2017bodyfusion,yu2018doublefusion,xu2019unstructuredfusion,su2020robustfusion,slavcheva2017killingfusion,gao2019surfelwarp,dou2016fusion4d}技术也发生了前所未有的进步。即使如此，三维重建结果无法满足虚拟现实、混合现实、动画、影视、游戏等领域的需求，一方面是重建几何模型存在瑕疵；另一方面，即使几何模型结果满足基本需求模型外观也无法达到与照片相媲美的程度，很难使人满意也无法直接应用。\par

一个高保真的纹理映射模型应该符合两个标准：精确的重建模型、清晰且全局一致的纹理。然而这两个标准常常被以下因素干扰。首先，深度相机采集的深度并非绝对准确，当测量距离大于一定阈值时会存在大量误差，而且在室内也会受光源影响使得度量误差增大。然后，对于几何模型现有的三维重建算法为了抵抗噪声在重建表面过程中用加权平均方法，但是会抹去几何模型的高频细节使得重建模型整体较为完美但是细节处存在瑕疵。再次，由于各种误差存在，即使存在回环检测、位姿图优化等算法，相机位姿也只能保证相对准确。以上各种因素累加后采集的图像和几何模型就无法准确对齐，要想获取高质量、高保真的纹理需要进一步做优化。\par


先前的工作采用各种方法试图解决以上所述问题，一般通过合成纹理图像~\cite{bi2017patch}、调整相机位姿~\cite{zhou2014color}或者细分网格顶点~\cite{ChengleiWu2014RealtimeSR}来生成更高质量的三维模型。有的工作不止着眼于单个组件的优化，通常在先前工作基础上额外提出优化策略使得纹理映射结果更加逼真。如G2Tex~\cite{fu2018texture}采用在基于面投影方法上额外使用从全局到局部的相机位姿矫正方案成功消除了纹理边界处的大量细缝，可以生成照片级纹理图像取得了良好的效果，seamless~\cite{fu2021seamless}也采用面投影方式，并额外采用块合成的方法在纹理边界处重新合成图案以消除纹理。大部分工作聚焦于如何改进或者生成纹理图像以弥补重建模型的误差，但是补偿能力有限只适用于重建模型误差较小情况，而且未考虑纹理、相机位姿、几何模型之间的耦合性关系，在优化场景选择上有限制。Intrinsic3D~\cite{RobertMaier2017Intrinsic3DH3}是首个基于阴影恢复形状（Shape From Shading，SFS）方法，对场景中的几何模型、纹理、相机位姿以及场景光照进行联合优化，但是由于SFS方案会产生纹理复制现象制约了Intrinsic3D方法的应用。最近fu等人提出JointTG~\cite{YanpingFu2020JointTA}方案对几何、相机位姿、纹理分别进行迭代优化，再采用分层方案代替混合优化策略成功解决Intrinsic3D纹理复制问题并且速度更快。但其复杂的框架分别针对几何、纹理和相机姿势使用不同的优化方案和目标函数，使得其可扩展性和健壮性较差，特别是在重建误差较大的情况下。\par


受可微分渲染在三维单视图模型重建~\cite{liu2020general}~\cite{ShichenLiu2019SoftRA}中的成功启发,我们将对抗生成网络与可微分渲染结合，提出一个基于可微分渲染的联合优化深度学习框架，同时对相机位姿与纹理，细化和重建模型进行联合。克服现有只对一个因素进行优化而难以达到全局最优解的障碍,并最终得到带有高质量几何细节和高保真纹理的三维重建结果。我们联合框架核心是可微分渲染模块，从纹理模型渲染至图像需要几何模型、纹理、相机位姿的共同参与，很容易使得梯度在各个组件之间进行更新传递，而且使用共同的图像间的损失来优化不同的组件，大大提高了可扩展性和灵活性。不仅如此，在优化几何模型的同时，为了突出恢复模型的高频细节，我们额外引入自适应质心细分方法，即只细分纹理丰富区域的几何模型。相比于平凡地优化几何模型，我们的方法在能恢复出几何模型的细节方面具有优越性。为了加快收敛速度与提高优化效果，我们提出了交替优化方案，分层迭代更新每一个组件，降低模块之间的耦合性，提高优化过程中的稳定性。\par

我们的实验表明，与最先进的方法相比，我们的联合优化框架相比于最新纹理优化方法在合成数据上的定量和真实数据上的定性上都产生了显著的性能提高。也展示了我们在恢复出三维重建模型高质量的几何细节和高保真的纹理细节方面具有优势。\par
总的来说，本章主要包括以下几个贡献：\par
\begin{enumerate}
\item 我们提出了基于可微分渲染的新的联合优化方案，将几何模型、纹理图像和相机位姿共同纳入统一的框架内，对每个组件进行交替优化，最终得到高质量的几何模型与高保真纹理的图像。
\item 我们采用对抗生成网络进行纹理图像重合成以增加真实感，渲染模块可以灵活嵌入神经网络中，并且网络结构可以灵活选择、优化过程灵活选择，提升了可扩展性。
\item 在优化网格顶点时，我们提出自适应细分方法，只在纹理丰富区域进行细分，减少代价同时达到与整体细分一致的效果。
\end{enumerate}


\section{相关工作}
\subsection{纹理优化}
为了恢复三维重建模型高质量的几何细节或高保真的纹理细节，近十几年，在三维重建领域做出了许多努力。我们从影响纹理优优化结果的不同因素:不精确的相机位姿、几何模型和纹理出发,按不同的优化思路把相关的算法分为一下几类。\par
\vspace*{2mm}\noindent{\bf 基于投影的方法：}它为每个面片选择最佳视角Lempitsky等人~\cite{lempitsky2007seamless}使用成对儿的马尔可夫随机场为每个三角面选择最优图像。这种方法面临一个具有挑战性的问题，即如何减轻相邻纹理之间的视觉接缝。Waechter等人~\cite{waechter2014let}提出了一种全局色彩调整算法，以减少由于视图投影造成的视觉破裂。Fu等人~\cite{fu2018texture} 提出了一种全局到局部的非刚性优化方法来调整摄像机的姿态漂移，并纠正几何误差引起的纹理坐标漂移。\par
\vspace*{2mm}\noindent{\bf 基于wrap的方法：}基于wrap的方法能够抵抗几何误差和相机漂移引起的对准误差问题。Zhou等人~\cite{zhou2014color}设计了一个纹理映射框架，通过局部图像扭曲来纠正相机的姿态和几何误差。但该方法需要对网格模型进行细分，这将大大增加数据量，限制其应用范围。\par
基于块合成的方法：Bi等人~\cite{bi2017patch}采用基于 patch 的图像合成方法来生成一个新的完全对齐的目标纹理图像 来消除纹理图像之间的不对齐，从而避免纹理结果的模糊和重影。\par
\vspace*{2mm}\noindent{\bf 基于联合优化的方法：}Robert Maier等人~\cite{RobertMaier2017Intrinsic3DH3}提出了一种基于SFS（shape-from-shading）和空间变化的球谐光照函数的子体优化方法，同时优化几何、纹理、相机姿态和场景照明。可以获得纹理一致的高质量三维重建。但该方法依赖于SFS，需要分解场景的光照，容易导致纹理拷贝问题。最近工作中Fu等人~\cite{YanpingFu2020JointTA}提出根据颜色和几何一致性以及高频法线线索对重建网格进行优化，有效地克服了SFS产生的纹理拷贝问题，从而得到了更加高质量的重建结果。\par
\vspace*{2mm}\noindent{\bf 基于深度学习的方法：}Huang等人~\cite{JingweiHuang2020AdversarialTO}使用从弱监督视图中获得的条件对抗损失为近似表面生成逼真的纹理，使用基于学习的方法训练纹理目标函数，以保持对摄像机姿态和几何畸变的鲁棒性。与我们相似的工作zhang等人~\cite{9705143}借助于可微分渲染方法提出了一种联合优化方法，将几何、纹理和相机姿态共同纳入一个统一的优化框架中，并采用一种自适应交织策略，提高优化的稳定性和效率。与我们的方法类似但是我们的方法在优化几何时采用自适应性细分，从而当几何误差过大时，我们的方法仍能恢复出到几何模型的细节，并能获取高保真的纹理。
\subsection{可微分渲染}
可微分渲染是通过梯度设计，能够将渲染输出的梯度方向传播至三维实体，从而弥补了二维和三维之间的差距，同时可以允许神经网路在操纵渲染图像的同时优化三维实体，无需额外的三维标注。基于不同的三维实体如体素、点云、SDF、网格有不同的可微分渲然方法，因为网格能够表达三维模型的拓扑结构，又无需关注三维实体内部的构成，这种表示方式不仅灵活而且节省内存，所以本文只关注基于网格的可微分渲染。
Loper和Black~\cite{MatthewLoper2014OpenDRAA}设计了基于网格的通用框架OpenDR，近似可微渲染器。Kato等人~\cite{MatthiasNiener2013Realtime3R}提出了一种神经3D网格渲染器，用手工设计的函数来近似光栅化后向梯度。SoftRas~\cite{ShichenLiu2019SoftRA}以概率方式将每个像素分配给网格的所有面，保证前向后向操作均可微。pytorch3d~\cite{ravi2020pytorch3d}基于SoftRas方法，设计出通用的基于网格的可微分框架，不仅方便地更新几何模型而且还能操作纹理贴图。我们的方法基于pytorch3d，借助于可微分渲染框架，将梯度反向传播并更新场景中相机位姿，几何和纹理贴图。注意我们并未考虑场景中其他参数，例如材质，灯光等。\par
更多关于可微分渲染的详细介绍请参阅综述\cite{HiroharuKato2020DifferentiableRA}。

\section{算法流程}

我们的方法旨在通过RGB-D摄像机获得具有精细几何细节三维模型和高保真的纹理贴图，为此我们提出了一个联合优化算法。优化几何模型，恢复出几何模型高频的几何细节；优化纹理，使之重生成高保真度的纹理；优化相机位姿，矫正估计不准的相机位姿，图X展示了我们方法流程。\par
这个部分，我们会阐述我们所提出方法的详细步骤。令$P$表示纹理、$M$表示几何模型（网格表示）、$T$表示相机外参、$K$表示相机内参。$I_c$,$I_D$,$I_S$分别表示手持RGB-D拍摄的彩色图、纹理图以及利用标准的计算机图形学管线生成的阴影图。纹理、几何模型和相机位姿和我们的最终目标（即恢复出清晰具有保真度的纹理）高度相关，而且三者之间具有耦合性。值得注意的是渲染本身就需要纹理、相机和几何模型的共同参与，可以将其纳入一个统一的优化框架，这与我们的优化目标高度契合。我们借助于pytorch3d ~\cite{ravi2020pytorch3d}可微分渲染框架，依据pytorch3d渲染流程我们可以制定更加合理的优化策略即每个部分分别进行交替迭代优化。下一个部分我会详细描述在统一的优化框架内纹理、相机位姿和网格的各自优化过程。\par
\subsection{数据处理}
\noindent\textbf{输入。}RGB-D数据我们使用消费级深度相机在固定曝光和白平衡模式下拍摄。我们利用经典的VoxelHashing框架~\cite{MatthiasNiener2013Realtime3R}获取初始的网格模型。对于初始纹理，我们利用加权平均策略将顶点从多个视口反投影至纹理图集上以生成初始纹理。\par
\noindent\textbf{关键帧获取。}由于RGB-D相机是手持进行拍摄，拍摄过程中不可避免的出现抖动相机运动过快，往往会造成所拍摄图片发生模糊、失真等现象。为了根除这个现象，我们会根据 Crete 等人提出的图片的模糊度度量指标 ~\cite{FrederiqueCrete2007TheBE}评估每一帧。我们设置一个尺寸为5的滑动窗口，在每个窗口内选择模糊度最小的图片作为关键帧$KF$。 \par
\noindent\textbf{辅助视角选取。}收Huang等人~\cite{JingweiHuang2020AdversarialTO}的启发，我们为每个原始视角$T_s$选取一个辅助视角$T_t$,使得辅助视角重投影至原视角作为真例以此来监督渲染的图片。令$p_s$，$p_t$分别表示原始视角和目标视角的齐次坐标，则二者的对应关系可以描述为:
\begin{align}
	p_s\sim KT_{t\rightarrow s}D_tK^{-1}p_t
\end{align}
其中$D_t$表示目标视角的深度值。由于遮挡和深度图存在噪声的缘故，目标视口扭曲到原始角会发生未对齐和残差现象。为了防止残缺部分过大，我们为任意两个视口计算z方向夹角$\theta$，当$\theta\le15^{\circ}$时两个视角才被加入到位姿对儿集合中。

\subsection{相机参数优化}
在基于RGB-D的三维重建中一般使用光束平差法估计初始的相机位姿。由于存在噪声相机位姿估计存在累计误差，即使有回环检测也并不都能完全消除。使用不完美的相机位姿渲染图片，渲染图片和采集图片会发生错位现象。借助于可微分渲染框架我们可以朝着期望的方向对相机位姿进行优化。\par
渲染模型选择上，我们使用标准的图像渲染流程即光栅化和布林冯着色模型。渲染目标遵从以下目标：
\begin{align}
	C_i = (I_a + I_d) * P_i + I_s
\end{align}
其中$I_a,I_d,I_s$分别表示环境光、漫反射光和高光项，$C_i,P_i$分别表示彩色图像素和纹理像素。为了最大限度的还原具有保真度的纹理，我们只保留环境光。对于相机外参T，令$T = (R,t)\in \mathrm{SE} (3),R_i \in \mathrm{SO}(3)$并且$t\in\mathbb{R}^3$。 
由于我们的框架中所有模块都是可微分的，可以端到端的进行训练。每一次渲染我们会随机选择某个视角，在该视角下生成彩色图$\tilde{I}_c$深度图$\tilde{I}_d$以及阴影图$\tilde{I}_s$即$I_c,I_d,I_s = Render(M_0|T_i)$。我们首先使用光度一致性损失来优化关键帧的相机位姿
\begin{align}
	E_c = \left \| I_c - \tilde{I}_c  \right \|_1 
\end{align}
然而仅仅具有光度一致性损失不足以约束相机位姿以朝着我们所想的方向优化，在某些场景纹理单一或者纹理较少，相机仍会发生漂移现象。因此我们额外考虑几何损失来增强几何一致性。
\begin{align}
	E_d = \left \| I_d - \tilde{I}_d  \right \|_1 
\end{align}
受SoftRas~\cite{ShichenLiu2019SoftRA}启发我们也采用轮廓损失来对几何进行约束。这个阴影损失函数为：
\begin{align}
	E_s = 1 - \frac{\left \| \hat{I_s}\otimes I_s  \right \|_1 }{\left \| \hat{I_s}\oplus  I_s- \hat{I_s}\otimes I_s \right \| }_1  
\end{align}
其中$\otimes $和$\oplus $分别是按元素计算的乘法运算符和运算符。最后我们按照每个损失对优化相机参数的贡献来赋予每个损失不同的权重。
\begin{align}
	L_T = \lambda_c E_c + \lambda_d E_d +\lambda_s E_s
\end{align}经验上我们设置$\lambda_c = 0.1$,$\lambda_d = 1$,$\lambda_s = 1$。

摄影机校正摄影机姿势Ti由Euler角度表示中的旋转和平移向量组成，每帧产生6个参数，这些参数使用BundleFusion姿势进行初始化，并在关节优化期间进行细化。受[63]的启发，添加了6层RELU MLP形式的相机像素空间的附加2D变形场，以考虑输入图像中可能的扭曲或相机固有参数的不准确。请注意，此校正域对于每个帧都是相同的。在优化过程中，相机光线首先使用在使用摄影机姿势Ti变换到世界空间之前，从变形场检索的2D向量。最后，在校正后的射线上绘制采样点。

\subsection{结合自适应细分的网格优化}
仅仅优化相机参数不足以保证网格上任意顶点投影至每个视角得到一致颜色，尤其是在网格重建误差较大情况下。不仅如此，在重建三维模型时一般会选用加权平均方法来抵抗重建过程中的噪声，虽然这种方法卓有成效，但是会造成过平滑的效果致使网格失去几何高频细节。我们同样也借助于可微分渲染方法恢复出几何表面的高频几何细节。我们为网格上每个顶点施加偏移量来矫正几何误差。\par
仅仅只更新顶点位置不足以保证几何模型细节突出，因为几何模型本身就过于平滑。我们采用质心细分网格方法增加三角形面片数目，一方面能使得模型更加契合真是世界场景，另一方面能减小顶点优化时发生漂移现象。\par
即使如此，细分网格代价是巨大的，增加顶点面片数目会增加渲染时间，并且在一些含有平面较多且纹理单一的几何模型上细分视觉效果并不明显。我们经验地发现优化几何模型时，顶点移动频繁发生在纹理丰富的地方，而纹理较为单一时顶点移动并不明显。因此我们建议根据场景本身的纹理丰富程度来决定是否要细分的程度。具体的，我们遵循一下步骤:

\begin{enumerate}
	\item 用soble算子提取所有视角的梯度图$\nabla g_i$，作为细分面片的依据。
	\item 计算几何模型上每个面片$f_j$投影至每个可见视口$i$的面积$A_{ij}$,并求出面积总和$\sum_{i}^{n} A_{ij}$。
	\item 计算每个面片$f_j$采样概率$\sum_{i}^{n} A_{j} / \sum_{j}^{m}\sum_{i}^{n} A_{ij}$，其中m为几何模型中面片数量，n为视角数量。
	\item 按照概率对所有面片进行无放回随机抽样，并设定采样概率阈值，在阈值概率之上面片才会被选取。
	\item 对所选择面片$f_j$进行质心细分。
\end{enumerate}

我们在第四步随机抽样时，为了防止太多的面片不在查询集合中，我们使用中位数而不是平均值作为采样阈值。细分完成后，我们仍用同样的方式增加对应的纹理坐标，以保持顶点和纹理坐标的对应关系。\par
为了防止优化过程出现异常情况，如裂缝现象。我们先用meshlab~\cite{LocalChapterEvents:ItalChap:ItalianChapConf2008:129-136}剔除重复顶点和面片以及零面积面片,以保证几何模型的规范性。优化几何模型时仅仅有图像间损失是远远不够的，因为每次迭代中顶点移动不受约束，会破坏网格模型的拓扑结构，导致退化的三角形。因此我们在图像损失基础上增加了几何正则化项，拉普拉斯项、法线一致性项和$L2$项。\par
拉普拉斯项定义为顶点坐标减去其临近顶点的加权和，在优化过程中可以保持局部几何特征不变。在优化过程中，拉普拉斯损失项可以帮助模型收敛到一个更加平滑的解，从而提高几何模型优化的准确度和稳定性。它也可以有效抑制因噪声或不规则采样而产生的噪点，使得优化结果更加符合实际。令$V \in \mathbb{R}^{n\times3}$为存储顶点位置的矩阵，则拉普拉斯项定义为：
\begin{align}
	E_l = \sum_{i=1}^{n}\left \| LV \right \|_2 
\end{align}
其中$L\in\mathbb{R}^{n\times n} $是网格的拉普拉斯图。\par
在优化过程中为了防止顶点偏移太远我们用$L_2$正则化项来约束顶点偏移量。其中$\mathbf{v}_{i}$为初始网格的顶点坐标，$\widetilde{\mathbf{v}}_{i}$为当前网格顶点的坐标。
\begin{align}
	E_{r}=\sum_{i}^{n}\left\|\mathbf{v}_{i}-\widetilde{\mathbf{v}}_{i}\right\|^{2}
\end{align}
最后，为了保证网格具有平滑性，我们额外使用了网格法线一致性损失。即利用余弦相似度计算相邻面片的法线一致性。
\begin{align}
	E_n= \frac{1}{|\overline{\mathcal{F}}|} \sum_{(i, j) \in \overline{\mathcal{F}}}\left(1-\mathbf{n}_{i} \cdot \mathbf{n}_{j}\right)^{2}
\end{align}
其中$\overline{\mathcal{F}}$代表共享一条边的相邻三角形面片的集合。$i$,$j$表示任意一对儿三角形面片例如$f_i$,$f_j$的法线。\par
最终网格重建损失项表示如下：
\begin{align}
	L_M = L_T + \lambda_l E_l +\lambda_r E_r+\lambda_n E_n
\end{align}
在实际优化中我们分别设置$\lambda_l = 1000$，$\lambda_r = 1000$,$\lambda_n = 10$。
\subsection{纹理重合成}
仅仅只矫正相机位姿和网格顶点位置是不够的，优化结果并不能完全消除噪声。况且我们采用顶点加权融合方式生成纹理，仍存在模糊并且缺失细节。由Huang等人~\cite{JingweiHuang2020AdversarialTO}得到启发我们用对抗生成网络学习重建模型和拍摄的彩色图片错位的容忍度。我们使用基于$\pi$GAN\cite{chanmonteiro2020pi-GAN}的网络用像素重生成管线合成纹理图。我们使用辅助视角重投影至原视角$I_c^{B\to A}$为真例监督可微分渲染生成的彩色图假例以更新纹理。相应地对抗损失函数定义如下：
\begin{align}
	L_{a d v}=\log D\left(\boldsymbol{I}_{A}, \boldsymbol{I}_{B \rightarrow A}\right)+\log \left(1-D\left(\boldsymbol{I}_{A}, \boldsymbol{I}_{A}^{D R, t}\right)\right) 
\end{align}
由于生成模型的发展，图像重建和合成已经取得了显著的进展。尽管如此，真实图像和生成图像之间仍可能存在差距，特别是在频域。在这项研究中，我们表明，缩小频域的差距可以进一步改善图像重建和合成质量。我们提出了一种新的焦点频率损失，它允许模型通过对容易合成的频率分量进行降权来自适应地聚焦于难以合成的频率分量。该目标函数是对已有空间损失的补充，对由于神经网络固有的偏差造成的重要频率信息的损失提供了巨大的阻抗。
\begin{align}
	L_\mathrm{FFL}=\frac{1}{M N} \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} w(u, v)\left|F_{r}(u, v)-F_{f}(u, v)\right|^{2}
\end{align}

经过若干次迭代交替训练判别器$D$和纹理$P$后，判别器会识别渲染图片中存在的伪影模糊或者裂缝，纹理会重新生成像素以弥补渲染图片和真实图片之间错位现象，使得两者充分接近以愚弄判别器。经过优化后的纹理相比与用加权融合方法生成的纹理更加真实，更接近于真实拍摄的彩色图片。
\subsection{交替优化}
相似于之前工作，我们使用联合优化策略优化相机参数、几何模型和纹理。我们用相机-网格-纹理优化顺序进行。一方面是在相机和网格优化中纹理生成方法不同于神经网络的像素重生成方法，另一方面在矫正相机参数和网格后，对抗生成网络效果重生成的纹理更加贴近于真实世界场景。具体地我们采用外部循环方法分别优化参数集$(T,M,P)$。我们首先固定参数$(M,P)$，通过最小化损失函数$L_T$以优化每一帧的相机参数$T$至$T'$；其次，我们使用参数$(T',P)$最小化损失函数$L_M$优化几何模型$M$至$M'$；最后我们使用并固定前两次的结果参数$(T',M')$最小化对抗损失$L_{adv}$来优化纹理$P$至$P'$。\par
在时间和效率的权衡下我们重复外循环优化策略3次。并且我们遵从由粗到细的策略优化不同的目标。具体地，在每一次外部迭代$t\in \left \{ 1,2,3 \right \}$中我们用指数衰减方式控制内部迭代次数，即内部迭代次数为$\text{epoch}  =\frac{s}{2^{t-1}}$，其中s为每一个内部迭代初始的轮数。除此之外，学习率也会相应地进行减半操作。我们的经验发现，利用指数衰减测率，可以保证最终效果的同时显著减小优化时间。我们设置初始的优化相机参数、优化几何模型和纹理次数为50，50，100。

\section{实验}
\paragraph*{比较方法}
在这个部分我们会和最新和方法G2Tex~\cite{fu2018texture},JointTG~\cite{YanpingFu2020JointTA},ATO~\cite{JingweiHuang2020AdversarialTO},Intrinsic3d~\cite{RobertMaier2017Intrinsic3DH3}在公开的RGB-D数据集上并用他们在GitHub提供的源代码在进行比较。因为ADJOIN~\cite{9705143}没有公开代码可得所以不进行比较。然后在我们自己拍摄的RGB-D的数据集上进行比较，最后我们会展示消融研究以证明我们联合优化框架的有效性。我们整个实验结果都在单个NVIDIA GeForce RTX3090 24GB服务器上计算得出。


\paragraph*{评估指标}
为了对所生成的纹理进行定量的研究，我们采用多种指标来衡量纹理和真实图片之间的差异。因为没有准确的指标来直接衡量生成的纹理图集，所以我们仍旧将纹理渲染为图像，然后利用经典的图像的评估指标，如峰值信噪比（PNSR）、结构相似性（SSIM）和感知相似度（LPIPS）来侧面衡量纹理清晰度和保真度。注意，我们使用所有视角评估的平均值作为最后的结果。
\subsection{在公共数据集上进行评估}
我们首先在ATO~\cite{JingweiHuang2020AdversarialTO}发布的椅子数据集上数据集上数据集上进行实验比较。如图X所示，可以看出我们的方法得到了最佳的效果，ATO采用对抗神经网络方法合成纹理图，虽然能够容忍较小的相机误差和几何误差，但是在局部地方仍然有模糊现象。G2Tex采用面投影方法，并优化了相机位姿，能够产生的纹理整体比较清晰，但是局部地方会有明显裂缝出现，这一点纹理比较丰富的数据集上更加明显。我们额外进行了定量评估，生成模型后我们在所有视角下评估定量指标最后取平均值展示。如表所示，我们的结果在椅子数据集上的定量指标（如上所示的）均超过了，最新的纹理优化方法。\par
其次我们在Intrinsic3D~\cite{RobertMaier2017Intrinsic3DH3}发布的数据集上进行评估,我们和intrinsic3D方法和JointTG方法，进行实验对比。这两个方法都分别实现了一个联合优化框架，同时优化纹理和几何模型。如图XX所示，我们的方法在纹理优化和几何优化上都取得了更佳的效果。由于Intrinsic3D基于阴影恢复形状（SFS），恢复几何细节方面效果非常好，但是由于SFS固有的缺陷，intrinsic3d很容易出现纹理复制伪影，尤其是当视角非常稀疏的情况。JoinTG的看起来视觉效果非常好，但是在几何和纹理细节方面，我们的结果比JoinTG更好，尤其是在人物手部眼睛方面，我们的方法恢复的细节更多，看起来更接近于拍摄图片。\par
最后，我们比较各种基线方法在zhou等人~\cite{Zhou2018}发布的Fountain数据集，该数据集视角非常稀疏。在此数据集上，我们的方法可以很好的处理标准化数据集，如图所示。我们的方法在几何细节和纹理细节方面对比其他方法展示出优越性。如图红色框中所示，我们的方法能恢复出清晰的喷泉模型文字，产生一致的纹理。其他方法在局部区域要么会产生模糊伪影要么会产生不一致的边界如G2Tex。我们额外实现了定量评估，如表所示，我们的方法对重建误差进行矫正，再用像素重生成管线合成纹理图，对几何误差和相机位姿误差更具有鲁棒性。经过联合优化后重建出的纹理，渲染的图片更接近于真值。\par
\subsection{在我们自己拍摄的数据上进行评估}
\subsection{消融研究}
在这个部分，我们将研究方法中各个部分的有效性。为了更有说服力，我们选择真实场景中的数据集Fountain。首先我们移除各个部分，没有优化相机位姿，没有优化几何模型，没有对抗生成网络，不采用自适应细分的情况，最后我们还讨论自适应细分方法，质心细分和基于边的细分效果。\par
如图所示，我们展示了移除的各个部分效果。没有矫正相机位姿情况下，渲染图片和真实图片会有错位现象，借助于对抗生成我们仍都能得到清晰的纹理，但是缺乏细节。没有几何细化，看起来对纹理生成效果最小，但是会在细节处产生轻微的伪影。没有对抗生成网络时，无论是纹理细节还是整体视觉效果都明显下降，并在定量评估中我们同样得出了一致的结论。\par
我们采用自适应细分方法，在纹理丰富处产生更多的顶点，保证视觉效果的同时减少数据量。如图所示，我们分别基于面和基于边的做法，虽然基于边的细分能产生更加均匀的效果，但是会导致未细分面片和已细分面片的公共边上的顶点产生读书不平衡，而基于面的做法保持规整。当优化几何模型时，基于边的细分容易导致拓扑结构的破坏，即三角形变形为四边形从而产生奇怪的几何模型。
\noindent \textbf{运行时间}
我们的优化时间随着数据集帧数线性增长，我们的方法在ATO发布的数据集上每个场景花费平均63分钟，其中优化相机位姿、几何、纹理时间占比分别为30，40,30。
