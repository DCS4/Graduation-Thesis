% !TeX root = ../Template.tex
% 本LaTeX模板的一般使用说明

% \clearemptydoublepage
% \setcounter{figure}{0}
% \setcounter{table}{0}
% \setcounter{algocf}{0}
% \chapter{相关理论基础}

\chaptera{相关理论基础}
三维重建和纹理优化是一个结合计算机图形学、计算机视觉和代数等知识的领域。它包含多种复杂的理论。本文首先介绍了三维重建和纹理模型所使用的数据获取方式；接着，阐述了如何对重建的三维模型进行表示，以及各种表示方法的优缺点。然后，介绍了常见的纹理表示方式以及如何实现纹理与模型之间的映射，同时给出基于加权融合方法生成初始纹理的详细步骤。最后，本文描述了可微分渲染的基本原理，并介绍了可微分渲染技术在重建模型和纹理优化中的作用。
%
% 缺乏示例图/Kinect相机、小孔成像原理图
%
\section{数据获取}

深度相机是一种能够获取三维场景信息的相机。它通常使用激光或者红外干涉仪来测量距离，并且能够根据这些距离信息生成深度图像或者三维点云。本文实验数据集主要采用微软发布的 Kinect \emph{v}1 相机采集场景的深度图序列和彩色图序列，部分场景下用Kinect \emph{v}2深度相机采集。Kinect \emph{v}1采用结构光方法进行深度测量，通过投影仪发射伪随机散斑红外光点，然后收集被测物体采集结构光图像并计算出相机至物体表面距离。深度相机测量范围在0.5m $\sim$ 4.5m之间，并且随着距离的增加，测量精度有所下降。Kinect \emph{v}1相机分辨率只有640 $\times$ 480，彩色图像的低分辨率使其易于模糊。此外，相机对光源较为敏感，容易受到室内光源的干扰。由于相机硬件的不完美和这些干扰项的存在，采集的数据中可能含有噪声。因此，在三维重建中，通常需要在算法层面处理或纠正这些噪声对数据的影响。Kinect \emph{v}2 相机在第一代基础上做了改进，利用飞行时间（Time of Flight,TOF）原理测量物体深度值。该传感器向目标发射红外光线，通过接收反射光线并记录光脉冲的飞行时间，估算出物体深度。Kinect \emph{v}2 相机可以拍摄分辨率为 1920 $\times$ 1080 的高清彩色图像和深度值更准确的深度图像。由于彩色图像和深度图像的分辨率不同，因此在使用前需要对彩色图像的相机镜头和深度图像的相机镜头进行分别标定。得到采集的深度数据后，一般会将其存储在单通道无符号16位的\emph{png}图像中，单位为毫米。\par 
在深度图上，像素和三维模型上的顶点存在一一对应关系。为了准确描述三维物体的姿态和位置，通常需要使用相机坐标系、世界坐标系、图像坐标系、像素坐标系和模型坐标等不同的坐标系来描述物体的位置和形态。根据小孔成像原理，三维物体最终会映射到图像平面上。在将模型投影到图像平面时，首先将位于世界坐标系内的模型通过刚体变换转换到相机坐标系中，然后按照透视投影原理将顶点变换到图像坐标系的像平面上。其中，图像坐标系的物理尺度为米，坐标原点在图像中心。为了得到统一的尺度表示，不受传感器尺寸的影响，还需要将像平面上的点转化为像素坐标系，并通过畸变矫正处理得到最终的图像数据。不失一般性，设三维模型上一点$\boldsymbol{p} = (X_w,Y_w,Z_w)^\top $，定义刚体变换为$\mathbf{g}\left(\boldsymbol{p},\mathcal{T}\right)=\mathcal{T} \boldsymbol{p} = \boldsymbol{R} \boldsymbol{p}+\boldsymbol{t}$ 。
其中，$\mathcal{T}=\left(\boldsymbol{R},\boldsymbol{t}\right) \in \mathrm{SE} (3),\boldsymbol{R}   \in \mathrm{SO}(3) \text { 和 } \boldsymbol{t} \in \mathbb{R}^{3}$。刚体变换后得到相机坐标系中一点$\boldsymbol{p'}=(X_c,Y_c,Z_c)^\top$。则$\mathrm{u}$为在图像平面$\mathrm{I}$上的投影为:
\begin{equation}
 \mathbf{u}\left(X_c,Y_c,Z_c\right)=\left(\frac{X_{c} f_{x}}{Z_{c}}+c_{x},\frac{Y_{c} f_{y}}{Z_{c}}+c_{y}\right)^{\top}
\end{equation}
其中，$f_x$和$f_y$表示焦距长度，$\left(c_{x},c_{y}\right)^{\top}$表示相机光心位置。最终得到二维像素位置$\boldsymbol{x}=(u,v)^{\top}$。设相机内参为$K$，则三维空间至图像平面的映射关系如下列公式所示：
\begin{equation}
\left(\begin{array}{c}
u \\
v \\
1
\end{array}\right)=\frac{1}{Z}\left(\begin{array}{ccc}
f_{x} & 0 & c_{x} \\
0 & f_{y} & c_{y} \\
0 & 0 & 1
\end{array}\right)\left[\begin{array}{cc}
\boldsymbol{R} & \boldsymbol{t} \\
\mathbf{0}^{T} & 1
\end{array}\right]\left(\begin{array}{c}
X_{w} \\
Y_{w} \\
Z_{w} \\
1
\end{array}\right)=\frac{1}{Z} \boldsymbol{K} \mathcal{T}\left(\begin{array}{c}
X_{w} \\
Y_{w} \\
Z_{w} \\
1
\end{array}\right)
\end{equation}

当相机透镜产生畸变时，它会影响成像平面上的像素位置和图像的形状，使得相邻像素之间的距离不再均匀。这种现象称为相机透镜畸变。如果在三维重建中使用了畸变图像，将会导致误差的积累影响重建结果的精度和质量。为了避免这些问题，需要对相机透镜进行标定和畸变矫正。确定相机内部和外部参数，包括焦距、光心、透镜畸变等，用于后续的三维重建。相机畸变可分为径向畸变和切向畸变。径向畸变是一种常见的畸变类型，光线在穿过透镜时在径向方向上发生弯曲，导致物体形状变形，尤其在图像边缘处更为明显。另一种类型是切向畸变，由于透镜材质和折射率等制造工艺缺陷导致光线在切向方向上发生弯曲，导致图像像素列倾斜或扭曲，通常在图像中心位置出现。对于径向畸变，可以通过泰勒级数进行矫正。而对于切向畸变，可以使用查找表或多项式方法进行校正。通过相机标定和畸变校正，可以提高三维重建的精度和可靠性，减少误差的积累，从而得到更加准确的重建结果。
\vspace{-2em}

\begin{equation}
\begin{array}{l}
x_{\text {corrected }}=x\left(1+\mathrm{k}_{1} \mathrm{r}^{2}+\mathrm{k}_{2} \mathrm{r}^{4}+\mathrm{k}_{3} \mathrm{r}^{6}\right) \\
y_{\text {corrected }}=y\left(1+\mathrm{k}_{1} \mathrm{r}^{2}+\mathrm{k}_{2} r^{4}+\mathrm{k}_{3} \mathrm{r}^{6}\right)
\end{array}
\end{equation}
其中，$x,y$为经过透视投影后的图像位置坐标，$r$为坐标点距成像中心的距离。切向畸变假设透镜和图像平面之间存在一个小的平移量，可以通过平移参数进行矫正。以下是切向畸变矫正公式。
\begin{equation}
\begin{array}{l}
x_{\text {corrected }}=x+\left[2 p_{1} y+p_{2}\left(r^{2}+2 x^{2}\right)\right] \\
y_{\text {corrected }}=y+\left[2 p_{2} x+p_{1}\left(r^{2}+2 y^{2}\right)\right]
\end{array}
\end{equation}

由上述公式共有$k_1,k_2,k_3,p_1,p_2$五个参数，求解参数过程就是图像去畸变过程，具体原理可参考相机标定原理，如经典的棋盘格标定\upcite{888718}。

%
% 缺乏示例图，TSDF、面元表示，网格模型图
%
\section{表面表示}
纹理是三维模型的视觉外观的具象化表示，它依赖于三维模型而存在。对于三维模型的表达方式，通常会根据构成元素的不同进行分类，具体选用哪种表面表示方式取决于重建算法和应用场景。常见的三维模型表示方式包括显式表示和隐式表示。显式表示方式包括点云、网格和体素等，而隐式表示方式包括占据网格和符号距离场等。选择不同的表面表示方式会影响到重建的效率、精度和所需存储空间等方面。\par

在SLAM重建过程中，为了满足实时重建要求，一般采用隐式表示方法中的截断符号距离场或面元素（Surfel）。为了建模真实场景，首先需要定义建模的三维空间。例如，KinectFusion\upcite{RichardNewcombe2011KinectFusionRD} 使用物理尺度下大小为$7m \times 7m \times 7m$的立方体表示室内空间。然后将三维空间划分为$128 \times 128 \times 128$个立方体小块，每个小块作为一个体素。随着分辨率的提高，体素数量急剧增加，相应的计算代价和内存代价呈指数级提高。因此，普通的体素表示方法不适用于大场景物体的重建，更适用于小场景的重建\upcite{nguyen2018rendernet}。由于体素表示相对规整，很容易进行数据更新和融合，并且易于进行GPU并行化处理，因此在基于RGB-D 的三维重建中，常使用体素的隐式表示法，例如截断符号距离场（Truncated Signed Distance Function,TSDF）。在TSDF模型中，每个体素小块存储相对于物体表面位置的距离值，正值表示在表面外，负值表示在表面内。由于重建过程中只关处于物体表面附近的体素，不需要考虑所有体素，因此计算效率大幅提高。\par


TSDF算法\upcite{BrianCurless1996AVM}可分为三个步骤：第一，需要对整个三维场景划分为$N$个规则的网格以表示整个场景。TSDF中每个网格存储距离值$D(p)$和相关权重$W(p)$，其中$p$为体素空间中的一个网格。第二，计算当前深度帧$\mathcal{Z}_{i}$上的TSDF值$d_i(p)$和权重$w_i(p)$。设$p$为单个体素在世界坐标系上的位置$p = (X_w,Y_w,Z_w)^\top$，根据上述介绍的投影流程获取在相机坐标系下z分量和投影至深度帧上的像素所代表的深度值后作差。则$d_i(p)$可表示为：
\begin{equation}
d_{i}(\boldsymbol{p})=\Psi\left(\left(\mathcal{T}_{i} \boldsymbol{p}\right)_{z}-\mathcal{Z}_{i}\left(\pi\left(\mathcal{T}_{i} \boldsymbol{p}\right)\right)\right) 
\end{equation}
其中，$\Psi(x) =\max \left(-1,\min \left(1,x\right)\right)$和投影函数$ \pi: \mathbb{R}^{3} \mapsto \mathbb{R}^{2}$。当前帧权重可表示为：
\begin{equation}
w_i(p) = \frac{ \cos \theta} { \left(\mathcal{T}_{i} p\right)_{z}-\mathcal{Z}_{i}\left(\pi\left(\mathcal{T}_{i}p\right)\right)} 
\end{equation}其中，$\theta$表示物体表面法向量与$p$到点光源连线的投影光线的夹角。
第三，基于当前帧所得的结果融入到全局模型结果中。当前帧的体素依赖于上一帧，整个体素的TSDF值为所有帧的权重乘以得到的TSDF值再平均。体素更新公式如下：
% \vspace{-1ex}
% \begin{align}
% \begin{array}{l}
% D_{i}(p)=\frac{W_{i-1}(p) D_{i-1}(p)+w_{i}(p) d_{i}(p)}{W_{i-1}(p)+w_{i}(p)}  \vspace{2ex}  \\
% W_{i}(p)=W_{i-1}(p)+w_{i}(p)
% \end{array}
% \end{align}


\begin{equation}
D_{i}(p)=\frac{W_{i-1}(p) D_{i-1}(p)+w_{i}(p) d_{i}(p)}{W_{i-1}(p)+w_{i}(p)}
\end{equation}
\vspace{-10ex}

\begin{equation}
W_{i}(p)=W_{i-1}(p)+w_{i}(p)
\end{equation}

\noindent 其中，$D_i(p)$表示融合第$i$帧后的体素加权融合结果，$W_i(p)$代表相关权重。融合所有深度帧后可以得到全局模型。之后可以用光线投射（ray casting,RC）算法\upcite{511}或者移动立方体（Marching cubes,MC）算法\upcite{lorensen1987marching} 抽取出三维网格模型。
在基于RGB-D相机的静态三维重建中，ElasticFusion\upcite{ThomasWhelan2015ElasticFusionDS}使用面元素（Surfel）表示法。每个Surfel代表一个小的面片，包含面片位置、面片方向、颜色、权重、半径以及时间戳等信息。Surfel的大小和密度由表面曲率决定。相比于其他表示方法，Surfel表示法不需要考虑重建模型表面的拓扑结构，因此可以通过增加、删除和移动Surfel来修改表面。正是由于面元素表示方法的灵活性，ElasticFusion可以优化重建地图并提高重建和位姿估计的精度。\par

除了SLAM中的两种表面表示法，基于学习的三维重建中也使用点云、三维网格、体素的隐式表示方法进行非实时的重建。Lars Mescheder等人\upcite{LarsMescheder2018OccupancyNL}使用占据网格表示法，每个体素包含一个二进制占据状态（占据/未占据），隐含地将三维表面定义为分离两种状态的分类决策边界。理论上可以表达三维模型无限制的分辨率，且不需要过多的内存占用。有效解决了原始体素表示存在的高内存低分辨率的缺陷。相似地，以上介绍的符号距离场（Signed Distance Function,SDF）表示方式也很契合多层感知机（Multi-layer Perceptron,MLP）。它可以学习二维连续决策边界表示三维物体的等值面，这种表示不需要占用很多内存，也不需要很深的网络模型就可以表示高分辨率的三维场景。因此，NeRF\upcite{mildenhall2021nerf}、DeepSDF\upcite{park2019deepsdf}等可以低成本代价获取高质量的三维模型。此外，基于SDF表示法易于与光线追踪算法\upcite{hart1996sphere}结合。神经网络更容易找到决策边界即表面位置，提升重建效率，间接获得高保真的重建模型，如Deepvoxels\upcite{Deepvoxels}。点云作为原始的三维表面表示法，使用点的分布表示三维场景。好处是很容易利用传感器进行获取，并只需关注点集的三维坐标，不需要关注点之间的连接关系和表面拓扑结构。PointNet\upcite{qi2017pointnet} 和 PointNet++\upcite{qi2017pointnet++} 是基于点云的表达方法，被广泛应用于点云处理领域。它们使用最大池操作来提取全局形状特征，作为点生成网络的编码器，能够用于深度学习的分类和分割任务。PointNet++ 相比于 PointNet 具有更高的精度和更少的计算量。\par


然而，基于点云的表示法缺乏拓扑信息，生成三维网格变得不那么容易，往往使用泊松重建算法\upcite{kazhdan2006poisson}后处理生成三维网格，无法产生水密表面。基于网格面片表示（三角形面片或四边形面片），可以表达三维模型的表面拓扑结构，且无需关注三维空间内部，比较灵活，而且相比体素表示更节省内存。相比于点云，网格表示法不仅考虑点的三维空间坐标也关注点之间的连接关系，适用于产生水密表面。由于网格表示更容易进行存储、编辑，而且有较多的相关开源软件，如MeshLab\upcite{LocalChapterEvents:ItalChap:ItalianChapConf2008:129-136}进行操作，因此基于网格的表示方法在实际应用中最广泛。但是基于网格表示的三维重建算法并不那么通用，因为大多数方法移动网格顶点或者重新生成网格面片时候会产生自相交的面片。而且只能生成简单拓扑结构的物体，并且需要预设置固定拓扑的模板网格来作为初始的重建模型。究其原因，网格在发生形变过程会改变顶点之间的连接关系，自然而然地造成拓扑结构的变化。此外，重建过程中无法准确预测顶点数量和连接关系，只能尝试多次网格细分增加面片数量。最后，网格模型的拓扑结构复杂多变，不像二维图像那样规则，也无法契合深度神经网络。因此，基于学习的重建方法只适用于重建拓扑结构简单且较小的物体。\par
本文在重建过程中选择了网格表示方法，因为它更适合纹理图集，并且易于进行纹理编辑。SLAM重建完成后，本文用Marching cubes算法\upcite{lorensen1987marching}抽取三维网格。然而，在重建过程中，遮挡和噪声等因素会导致重建出的网格表面存在残缺，不符合2D流形规则，这给纹理优化步骤带来了新的问题和挑战。

%
%缺乏示例图，各种纹理图
%
\section{纹理表示与获取}
\subsection{纹理表示}
纹理作为三维模型的外观表示与三维模型密不可分。为便于编辑，纹理通常以图像形式存储，并保留三维模型与纹理的映射关系。通俗地说，纹理是一种表达每个模型顶点颜色信息的表示。纹理不仅包含像素颜色，还维护了各个像素的位置与顶点之间的对应关系。使用纹理可以为三维模型增添有趣的外观，即使模型本身并不复杂，利用纹理可以伪造模型的几何细节，使其看上去更加丰富。例如，在游戏中常常使用纹理来展示丰富的场景，但计算代价并未增加。如竞技类游戏中的人物皮肤、射击类游戏中的枪械皮肤、墙壁贴图等。纹理通常以2D图片形式存在。根据纹理的表示方式，纹理可以进一步分为一维纹理（只关注某一个方向上的纹理）；二维纹理（平面方向上的纹理）；三维纹理（空间方向上的纹理）。相比于彩色图像，纹理反映了图像的本质特征，并不依赖于图像颜色或亮度变化，刻画了图形区域的像素灰度级空间分布属性。在纹理重建/优化中，通常使用以下表示方式。\par
\vspace*{2mm}\noindent{\bf 顶点纹理：} 纹理可以作为顶点颜色的表示，在存储三维模型时，最简单的方法是将对应顶点的颜色也直接存储在内部。这种表示方法非常适用于包含纹理网格细分的纹理优化算法，例如 Colormap\upcite{zhou2014color} 和 Intrinsic3D 算法\upcite{RobertMaier2017Intrinsic3DH3}。但是，为了保证图形的真实性，必须有足够多的顶点来指定足够多的颜色。这显然会增加存储开销，并且不易转换为其他纹理表示方式。\par
\vspace*{2mm}\noindent{\bf UV纹理：}这种表示方法使用彩色图像保存纹理，并使用\emph{mtl}文件存储所有纹理图像路径和材质信息。在纹理优化算法中经常使用此表示方法。相比于顶点纹理，它还可以额外指定材质参数，如环境光、高光、漫反射光系数、滤光透射率等。存储几何模型时无需额外存储顶点颜色信息，但会存储每个顶点的纹理坐标。以模型存储格式 \emph{obj} 为例，文件将存储顶点的空间坐标、顶点之间的连接关系以及每个顶点所对应的纹理坐标。假设模型上一点 $p$ 对应的纹理坐标为 $s = (u,v)$，其中 $u,v \in (0,1)$。假设纹理图片的大小分别为 $h$ 和 $w$，那么 $p$ 对应纹理图像上的像素位置为 $(h \times u, w \times v)$。由于在构建几何模型时没有考虑纹理坐标，因此只能通过特定的纹理图像生成算法来设定合适的纹理坐标位置。改变纹理图像必须相应的改变纹理坐标位置，这限制了应用范围。\par

\vspace*{2mm}\noindent{\bf 图集纹理：}这种表示方法比较特殊，来自于\upcite{ShichenLiu2019SoftRA}。相对于UV纹理，它不再将整个三维模型的纹理信息储存在一张图片中，而是预先定义多个正方形纹理块，让每个纹理块表示模型中特定的三角形面片纹理。这种表示方法更加适用于类似于shapenet\upcite{shapenet2015}数据集的场景，因为某些模型的部分面片可能没有纹理信息。此外，这种方法可以方便地和UV纹理进行转换，具有很好的灵活性。\par
\vspace*{2mm}\noindent{\bf 立方体纹理：}使用立方体的六个面来表示场景纹理。一种常见的纹理映射方法是使用球面UV映射，同时使用cubemaps\upcite{greene1986environment}来可视化球面领域。使用立体图来记录从一个单位球体投射出来的颜色信息，由一个单位盒子的六个面组成，因此被广泛用于图形学中的球形映射。另外一种标准的方法来可视化球形函数是使用等角图（equirectangle map）。相比于等角图，立方体地图失真较少，可以避免等角图在顶部和底部产生的失真区域。更多关于纹理表示的介绍可以参考\upcite{tarini2017rethinking,yuksel2019rethinking}。\par
\subsection{纹理获取}
在估计了每帧彩色图像对应的相机位姿之后，可以使用图形学渲染管线，将三维模型的所有面片或顶点投影至所有彩色图上，并通过可见性测试获得每个面片或顶点在可见范围内的颜色信息。如果是面片，可以只投影至一张最清晰的彩色图像上，然后将结果存储在纹理图中，同时保留三维模型顶点与图像像素之间的映射关系；如果是顶点，可以通过加权平均或取置信度最高的投影位置像素的方式，融合所有投影位置的颜色值来得到该顶点的最终颜色信息。最后，可以将生成的纹理信息直接存储到以\emph{ply}格式的三维模型中。为了方便进行模型的投影，本文使用了已经预设好纹理坐标的网格模型。在本文的第一和第二个工作中，生成初始纹理图像的具体算法流程如下：\par



\begin{enumerate}[label=(\arabic*),leftmargin=\parindent,align=left,labelwidth=\parindent,labelsep=0pt]
    \item 预定义一张空白纹理图片$P$，$1024 \times 1024$大小。
    \item 设纹理坐标一点$U = (u,v,1)^\top$，将纹理坐标$u$利用相机外参$\mathcal{T}$与自定义内参$\mathcal{K}$投影至纹理图$P$上，得到像素点$X=(x,y)$。其中$X = U\mathcal{T}\mathcal{K}$。
    \item 记录覆盖像素点$X$的三角形面片$F$,记录面片索引$i$，计算像素$X$在图像平面上投影三角形的重心坐标$b_X\in(0,1)$坐标范围为$0 \sim 1$之间。
    \item 根据像素重心坐标$b_X$和面片索引$i$，计算出像素对应的顶点$v$和法线$\vec{n}$。
    \item 利用上一步求得的顶点$v$和面片$F$，向不同视角投影，求出顶点在视角$j$上投影点的像素颜色$c_j$，然后加权平均后获取最终颜色值$c= \frac{1}{N}  \sum_j^N c_j $，N为视角总数。
    \item 将所有顶点颜色存储到空白纹理图$P$上，将图片路径存储至\emph{mtl}文件中。
\end{enumerate}
重复上述步骤直到所有纹理坐标都投影完毕。 





\section{可微分渲染技术介绍}
渲染来自于计算机图形学，按照渲染方法可以细分为光栅化、光线追踪、体渲染等。光栅化渲染算法对每一个三角形面片进行单独渲染，可以在屏幕上快速显示3D图像，但是它不能处理高级的光照和阴影效果；光线追踪算法是一种基于物理光学原理的渲染技术，它可以模拟光线在3D空间中的传播、折射、反射等现象，实现高质量的图像渲染。在光线追踪算法中，光线从相机位置出发，经过每个像素，并沿着场景中的物体表面传播，最终到达光源或被吸收。通过对光线和物体的交互关系进行计算，算法可以计算出每个像素的颜色值和明暗程度，实现逼真的光影效果和高质量的全局光照效果。但是它需要较长的计算时间，常用于真实感图形渲染。体积渲染也是计算机图形学中的一种技术，用于在三维场景中模拟物体的材质和光线。它通过在每个像素上模拟光线与场景中的对象的交互来生成图像。与其它渲染技术不同，体积渲染可以模拟物体内部的材质和光线，并且可以生成逼真的阴影和渐变效果。常用的体积渲染技术有雾化、半透明渲染和烟雾效果等。由于以上算法渲染过程并不可微分，无法直接嵌入深度学习框架中。为了能用梯度下降算法更新场景参数，要么改变前向渲染过程\upcite{ShichenLiu2019SoftRA}使其可微分，要么手工设计后向传播的梯度\upcite{HiroharuKato2017Neural3M}。
神经渲染是一个可微分渲染中的前沿领域，它将机器学习技术与计算机图形学和物理学知识相结合，旨在通过训练神经网络来建模场景中几何、光照和材质之间的复杂关系，从而生成逼真的图像或视频。与传统渲染技术不同的是，神经渲染可以更灵活地生成视觉内容。通过利用可微分操作，例如神经辐射场或可微分路径跟踪。利用神经网络来代替传统的渲染器，以获得可控和逼真的输出。目前在三维重建\upcite{yariv2020multiview}或者新视角合成\upcite{mildenhall2021nerf}\upcite{zhang2020nerf++}领域已经取得了显著效果。更多关于神经渲染的介绍请参阅论文\upcite{tewari2020state}。\par
可微分渲染(Differentiable Rendering,DR)旨在通过获得渲染过程的有用梯度来解决端到端优化的集成问题。通过微分渲染，DR弥合了2D和3D处理方法之间的差距，允许神经网络在操作2D投影的同时优化3D实体，而无需收集三维实体的属性或者标注。具体地，利用渲染器，相机内外参以及三维模型渲染出彩色图片，然后与真实采集的图片计算其差异，并且反向传播误差以更新场景的各个参数。例如，顶点位置、材质、场景灯光和相机位姿等属性。现有的可微分渲染研究按照渲染质量可以分为两类：基于物理的渲染方法，专注于生成逼真的图像质量或者追求更高的性能，往往使用渲染方程\upcite{kajiya1986rendering}的微分近似来模拟真实世界的光的反射与材质属性，渲染模型更加符合真实世界光照。第二种是基于经验的可微分渲染方法，使用简单的着色模型，如Blinn-Phong光照模型\upcite{blinn1977models}来对真实世界光照做经验上的近似。可微分渲染器通过从投影像素到3D参数生成导数来近似梯度，相比基于物理的渲染，参数量少而且效率高。\par
可微分渲染是一系列技术合集，并不具体指某项技术。根据三维实体的表面表示，它可以细分为基于体素表示、基于隐式表示、基于点云表示或者基于网格表示的渲染方法。体素是三维空间中的单位立方体，其规则形状与三维卷积有天然的结合优势。Yang等人\upcite{nguyen2018rendernet}提出了一种基于3D卷积的可微分渲染卷积神经网络，它具有一种创新投影单元，可以将三维形状转化为二维图像。但由于体素的存储代价随着场景分辨率的提高而显著增高，因此重建场景大小受到限制。在基于学习的方法中，最常用的是体素隐式表示，其中占据网格和符号距离场等技术。如上一节所述，占据概率和截断符号可以隐式地表示为神经网络的决策边界，这样使用有限的内存代价就可以表达无限的分辨率。如\upcite{niemeyer2020differentiable,jiang2020sdfdiff,liu2020dist}等采用解析计算方法，使前向渲染过程可微分，从而获取高质量的渲染图像。近年来，在神经网络中以参数化的方式来表示几何信息越来越常见，这比三维显式表面表示更具有内存效率，可以充分利用GPU的并行处理能力。隐式表示的可微分渲染也逐渐成为主流。由于点云获取较为方便，因此成为三维数据表示的自然选择，而且点云可以方便地集成到深度神经网络中，渲染时通过汇聚点云上所有可见的点特征，转换为彩色图像像素，如\upcite{wiles2020synsin,lassner2021pulsar}等。

基于网格的渲染方法可以分为近似梯度、近似渲染和全局照明三类。通过改变前向或后向渲染的方式或者用梯度近似渲染三角形面片，可以将其渲染至图像中。但基于可微分渲染的推断几何模型形状方面，为了保持网格拓扑结构不发生变化，通常需要预先定义一个模板网格，限制了重建物体的范围。\par

本文利用基于网格的可微分渲染来更新和优化纹理、几何模型形状和相机位姿，而并不关注材质、灯光等属性。在考虑性能和效率的情况下，本文使用基于光栅化的渲染器pytorch3d\upcite{ravi2020pytorch3d}。该渲染器包含两个组件，一个是光栅化组件：它选择影响每个像素的三角形面片，另一个是渲染组件：它计算每个像素的颜色。光栅化步骤：使用刚体变换和透视投影将三角形面片投影到图像平面上，得到覆盖每个像素的一组三角形面片。在传统的光栅化中，每个像素仅受其沿z轴最近的三角形面影响。在可微分渲染中，将为这些三角形面赋予不同的概率，其中每个面片的概率与其覆盖像素位置的顶点z坐标的相对位置相关，距离相机光心的距离越近，概率就越大。最后，按照概率加权融合不同的三角形面片，形成像素值，作为最终的像素颜色。渲染算法：Blinn-Phong光照模型中，物体颜色来源于三种光的累加：环境光$I_a$、漫反射光$I_d$和高光$I_s$。这些光照项会乘以各自的相关系数，最终得出像素的颜色。
\begin{equation}
    L = k_aI_a + k_dI_d+k_sI_s
\end{equation}
其中，$k_a,k_d,k_s$分别表示环境光照、漫反射光照和高光项系数。

\section{本章小结}
本节主要介绍了基于RGB-D相机的三维重建和纹理优化的相关知识，包括数据获取、表面表示、纹理获取和表示以及可微分渲染技术等。三维重建利用RGB-D相机采集的彩色图和深度图，本文首先介绍了深度相机的特点，接着阐述相机成像模型，并推导了三维模型顶点从世界坐标系向像素坐标系转换的过程。最后，介绍了图像去畸变的原理。第二部分中，本文首先介绍了三维重建模型的不同表面表示方式，讨论了各种表面表示方法在不同重建场景下的应用和优缺点。第三部分，本文介绍了纹理的特点以及与三维模型之间的映射关系，并列举了在纹理优化过程中使用的不同纹理表示方法。最后，简要介绍了本文为三维模型生成初始纹理模型所使用的算法流程。在最后一部分，本文首先介绍了三种常见渲染方法的特点及应用场景。其次，阐述可微分渲染的基本原理，并讨论了现阶段可微分渲染发展中的两个分支及其各自的特点。然后，本文具体介绍了可微分渲染技术的种类及其应用场景。最后，介绍了本文工作一和工作二所使用的可微分渲染框架。
